---
title: "Streaming v√† Real-time Responses: T·ªëi ∆Øu User Experience"
date: 2025-05-24 09:00:00 +0700
categories: [AI, OpenAI SDK]
tags: [openai, agents, streaming, real-time, ux]
author: toantranct
description: H∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ Streaming trong OpenAI Agents SDK. T·ª´ raw response events ƒë·∫øn building responsive UIs - t·∫°o tr·∫£i nghi·ªám ng∆∞·ªùi d√πng m∆∞·ª£t m√† v√† real-time.
---

Trong th·ªùi ƒë·∫°i m√† ng∆∞·ªùi d√πng quen v·ªõi vi·ªác nh·∫≠n th√¥ng tin t·ª©c th√¨ - t·ª´ typing indicators tr√™n WhatsApp ƒë·∫øn live updates tr√™n social media - vi·ªác ch·ªù ƒë·ª£i agents "suy nghƒ©" m√† kh√¥ng c√≥ feedback n√†o c√≥ th·ªÉ t·∫°o c·∫£m gi√°c kh√≥ ch·ªãu. ƒê√¢y ch√≠nh l√† l√∫c **Streaming** ph√°t huy t√°c d·ª•ng.

Streaming kh√¥ng ch·ªâ l√† vi·ªác "hi·ªÉn th·ªã t·ª´ng t·ª´ m·ªôt" m√† l√† ngh·ªá thu·∫≠t t·∫°o ra **tr·∫£i nghi·ªám t∆∞∆°ng t√°c t·ª± nhi√™n** - cho ng∆∞·ªùi d√πng c·∫£m gi√°c nh∆∞ ƒëang tr√≤ chuy·ªán v·ªõi m·ªôt con ng∆∞·ªùi th·∫≠t s·ª±. H√¥m nay ch√∫ng ta s·∫Ω kh√°m ph√° c√°ch bi·∫øn agents t·ª´ nh·ªØng "black boxes" th√†nh nh·ªØng "ƒë·ªëi t√°c t∆∞∆°ng t√°c" responsive v√† engaging.

## T·∫°i Sao C·∫ßn Streaming?

### V·∫•n ƒê·ªÅ V·ªõi Non-Streaming Responses

H√£y t∆∞·ªüng t∆∞·ª£ng b·∫°n h·ªèi m·ªôt c√¢u ph·ª©c t·∫°p v√† ph·∫£i ch·ªù 30 gi√¢y m·ªõi c√≥ ph·∫£n h·ªìi:

```python
# ‚ùå Non-streaming - User experience k√©m
async def poor_ux_example():
    agent = Agent(name="Slow Agent", instructions="You provide detailed analysis...")
    
    print("User: Ph√¢n t√≠ch th·ªã tr∆∞·ªùng crypto trong 6 th√°ng qua")
    print("Agent: ‚è≥ ƒêang suy nghƒ©...")
    
    # User ph·∫£i ch·ªù 30 gi√¢y kh√¥ng bi·∫øt g√¨ ƒëang di·ªÖn ra
    await asyncio.sleep(30)  # Simulate long processing
    
    print("Agent: [Suddenly dumps a huge response]")
```

**üî¥ Problems:**
- User kh√¥ng bi·∫øt agent c√≥ ƒëang ho·∫°t ƒë·ªông hay kh√¥ng
- Kh√¥ng c√≥ indication v·ªÅ progress hay completion time
- C·∫£m gi√°c "ƒë·ª©ng h√¨nh", kh√¥ng responsive
- Kh√¥ng th·ªÉ cancel hay interrupt n·∫øu c·∫ßn

### L·ª£i √çch C·ªßa Streaming

```python
# ‚úÖ Streaming - User experience tuy·ªát v·ªùi
async def great_ux_example():
    print("User: Ph√¢n t√≠ch th·ªã tr∆∞·ªùng crypto trong 6 th√°ng qua")
    print("Agent: üí≠ ƒêang ph√¢n t√≠ch...")
    
    # Real-time feedback
    print("Agent: üìä ƒêang thu th·∫≠p d·ªØ li·ªáu th·ªã tr∆∞·ªùng...")
    await asyncio.sleep(2)
    
    print("Agent: üîç ƒêang ph√¢n t√≠ch trends...")
    await asyncio.sleep(2)
    
    print("Agent: ‚úçÔ∏è ƒêang t·∫°o b√°o c√°o...")
    # Then stream the actual response word by word
    response = "Th·ªã tr∆∞·ªùng cryptocurrency trong 6 th√°ng qua..."
    
    for word in response.split():
        print(word, end=" ", flush=True)
        await asyncio.sleep(0.1)  # Simulate streaming
```

**‚úÖ Benefits:**
- **Immediate feedback** - User bi·∫øt ngay agent ƒëang x·ª≠ l√Ω
- **Progress indication** - Shows what's happening at each step  
- **Natural flow** - Gi·ªëng nh∆∞ conversation th·∫≠t
- **Interruptible** - User c√≥ th·ªÉ stop n·∫øu c·∫ßn
- **Perceived performance** - Feels faster even if total time is same

## Streaming Architecture trong OpenAI Agents SDK

### Stream Event Types

OpenAI Agents SDK cung c·∫•p hai lo·∫°i streaming events:

```python
from agents import Agent, Runner
from openai.types.responses import ResponseTextDeltaEvent

# 1. üîß RAW RESPONSE EVENTS - Low-level, token-by-token
# 2. üì¶ RUN ITEM EVENTS - High-level, structured updates
```

**üîß Raw Response Events:**
- Token-by-token output t·ª´ LLM
- Real-time text generation
- Gi·ªëng nh∆∞ typing indicator
- Best cho text display

**üì¶ Run Item Events:**  
- High-level progress updates
- "Message generated", "Tool called", "Agent switched"
- Best cho UI state management
- Structured information v·ªÅ workflow

### Basic Streaming Implementation

```python
import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner

async def basic_streaming_demo():
    """Demo basic streaming v·ªõi token-by-token output"""
    
    agent = Agent(
        name="Storyteller",
        instructions="""
        B·∫°n l√† m·ªôt k·ªÉ chuy·ªán t√†i ba.
        
        Phong c√°ch:
        - K·ªÉ chuy·ªán h·∫•p d·∫´n, cu·ªën h√∫t
        - S·ª≠ d·ª•ng ng√¥n ng·ªØ sinh ƒë·ªông
        - T·∫°o suspense v√† drama
        - K·∫øt th√∫c c√≥ √Ω nghƒ©a
        
        H√£y k·ªÉ nh·ªØng c√¢u chuy·ªán ng·∫Øn nh∆∞ng ƒë·∫ßy c·∫£m x√∫c.
        """
    )
    
    # Request a story
    user_request = "K·ªÉ cho t√¥i m·ªôt c√¢u chuy·ªán ng·∫Øn v·ªÅ t√¨nh b·∫°n"
    print(f"üë§ **User:** {user_request}")
    print("ü§ñ **Agent:** ", end="", flush=True)
    
    # Stream the response
    result = Runner.run_streamed(agent, user_request)
    
    async for event in result.stream_events():
        # Handle raw text streaming
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            # Print each token as it arrives
            print(event.data.delta, end="", flush=True)
    
    print("\n")  # New line after story completion
    
    # Get final result
    final_result = await result
    print(f"\nüìä **Metadata:** Ho√†n th√†nh trong {len(final_result.raw_responses)} API calls")

if __name__ == "__main__":
    asyncio.run(basic_streaming_demo())
```

### Advanced Streaming v·ªõi Run Item Events

```python
from agents import Agent, Runner, ItemHelpers, function_tool
import random
import asyncio

@function_tool
def roll_dice(sides: int = 6) -> int:
    """Roll a dice v·ªõi s·ªë m·∫∑t t√πy ch·ªçn"""
    return random.randint(1, sides)

@function_tool
def flip_coin() -> str:
    """Tung ƒë·ªìng xu"""
    return random.choice(["Ng·ª≠a", "S·∫•p"])

@function_tool
def magic_answer() -> str:
    """C√¢u tr·∫£ l·ªùi th·∫ßn b√≠"""
    answers = [
        "C√≥ th·ªÉ", "Ch·∫Øc ch·∫Øn", "Kh√¥ng ch·∫Øc", 
        "H·ªèi l·∫°i sau", "Kh√¥ng", "Tuy·ªát ƒë·ªëi"
    ]
    return random.choice(answers)

async def advanced_streaming_demo():
    """Demo advanced streaming v·ªõi structured events"""
    
    game_master = Agent(
        name="Game Master",
        instructions="""
        B·∫°n l√† m·ªôt Game Master t√†i ba cho tr√≤ ch∆°i role-playing.
        
        Kh·∫£ nƒÉng c·ªßa b·∫°n:
        - Roll dice ƒë·ªÉ x√°c ƒë·ªãnh k·∫øt qu·∫£ h√†nh ƒë·ªông
        - Tung coin ƒë·ªÉ quy·∫øt ƒë·ªãnh ng·∫´u nhi√™n
        - ƒê∆∞a ra magic answers cho c√¢u h·ªèi kh√≥
        
        Phong c√°ch:
        1. Lu√¥n explain t·∫°i sao c·∫ßn d√πng tool
        2. Interpret k·∫øt qu·∫£ tool theo context
        3. T·∫°o narrative h·∫•p d·∫´n xung quanh results
        4. Encourage players ti·∫øp t·ª•c adventure
        
        Make m·ªói interaction th√†nh m·ªôt story moment!
        """,
        tools=[roll_dice, flip_coin, magic_answer]
    )
    
    scenarios = [
        "T√¥i mu·ªën leo l√™n v√°ch ƒë√° cao 20 m√©t. T√¥i c√≥ th√†nh c√¥ng kh√¥ng?",
        "C√≥ 2 con ƒë∆∞·ªùng tr∆∞·ªõc m·∫∑t. T√¥i n√™n ch·ªçn ƒë∆∞·ªùng n√†o?",
        "Li·ªáu t√¥i c√≥ t√¨m ƒë∆∞·ª£c kho b√°u trong hang ƒë·ªông n√†y kh√¥ng?"
    ]
    
    for scenario in scenarios:
        print(f"\nüé≤ **Scenario:** {scenario}")
        print("üé≠ **Game Master:** ", end="", flush=True)
        
        # Stream v·ªõi detailed event handling
        result = Runner.run_streamed(game_master, scenario)
        
        current_text = ""
        tool_calls_made = 0
        
        async for event in result.stream_events():
            
            # Raw text streaming
            if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                print(event.data.delta, end="", flush=True)
                current_text += event.data.delta
            
            # Structured event handling
            elif event.type == "run_item_stream_event":
                
                if event.item.type == "tool_call_item":
                    tool_calls_made += 1
                    print(f"\nüîß **[Tool {tool_calls_made}]** ƒêang s·ª≠ d·ª•ng {event.item.tool_name}...")
                
                elif event.item.type == "tool_call_output_item":
                    tool_output = event.item.output
                    print(f"‚ö° **[Result]** {tool_output}")
                    print("üé≠ **GM continues:** ", end="", flush=True)
                
                elif event.item.type == "message_output_item":
                    # Message completed - c√≥ th·ªÉ add special formatting
                    pass
        
        print("\n" + "="*60)
    
    print(f"\nüéâ **Adventure complete!** Total tools used across all scenarios: {tool_calls_made}")

if __name__ == "__main__":
    asyncio.run(advanced_streaming_demo())
```

## Building Responsive User Interfaces

### Console-based Chat Interface

```python
import asyncio
import sys
from datetime import datetime
from typing import List, Dict, Any
from agents import Agent, Runner
from openai.types.responses import ResponseTextDeltaEvent

class StreamingChatInterface:
    """Interactive chat interface v·ªõi streaming support"""
    
    def __init__(self):
        self.conversation_history: List[Dict[str, str]] = []
        
        self.chat_agent = Agent(
            name="Friendly Assistant",
            instructions="""
            B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán v√† h·ªØu √≠ch.
            
            T√≠nh c√°ch:
            - Nhi·ªát t√¨nh v√† t√≠ch c·ª±c
            - Th√¥ng minh nh∆∞ng kh√¥ng kh√¥ khan
            - C√≥ sense of humor nh·∫π nh√†ng
            - Quan t√¢m ƒë·∫øn user's needs
            
            Communication style:
            - S·ª≠ d·ª•ng emojis m·ªôt c√°ch t·ª± nhi√™n
            - H·ªèi follow-up questions khi c·∫ßn
            - Provide examples v√† context
            - Keep responses concise nh∆∞ng informative
            
            N·∫øu kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ th√¥ng tin, h√£y th√†nh th·∫≠t admit.
            """
        )
    
    def display_header(self):
        """Display chat interface header"""
        print("=" * 60)
        print("ü§ñ **STREAMING CHAT INTERFACE** ü§ñ")
        print("=" * 60)
        print("üí° Tips:")
        print("   ‚Ä¢ Type '/help' for commands")
        print("   ‚Ä¢ Type '/history' to see conversation")
        print("   ‚Ä¢ Type '/clear' to clear history")
        print("   ‚Ä¢ Type '/quit' to exit")
        print("=" * 60)
    
    def get_user_input(self) -> str:
        """Get user input v·ªõi prompt"""
        try:
            return input("\nüë§ **You:** ").strip()
        except KeyboardInterrupt:
            print("\n\nüëã Chat interrupted. Goodbye!")
            sys.exit(0)
    
    async def stream_agent_response(self, user_message: str) -> str:
        """Stream agent response v·ªõi visual indicators"""
        
        print("ü§ñ **Assistant:** ", end="", flush=True)
        
        # Add user message to history
        self.conversation_history.append({
            "role": "user", 
            "content": user_message,
            "timestamp": datetime.now().isoformat()
        })
        
        # Build context t·ª´ conversation history
        if len(self.conversation_history) > 1:
            # Include recent conversation context
            context_messages = []
            for msg in self.conversation_history[-6:]:  # Last 3 exchanges
                context_messages.append(f"{msg['role']}: {msg['content']}")
            
            contextual_input = f"""
Previous conversation:
{chr(10).join(context_messages)}

Current user message: {user_message}

Please respond naturally, considering the conversation context.
"""
        else:
            contextual_input = user_message
        
        # Stream the response
        result = Runner.run_streamed(self.chat_agent, contextual_input)
        
        response_text = ""
        typing_indicator_shown = False
        
        async for event in result.stream_events():
            
            # Handle raw text streaming
            if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                if not typing_indicator_shown:
                    typing_indicator_shown = True
                
                delta = event.data.delta
                print(delta, end="", flush=True)
                response_text += delta
            
            # Handle structured events for additional context
            elif event.type == "run_item_stream_event":
                if event.item.type == "tool_call_item":
                    print(f"\n   üîß [Using tool: {event.item.tool_name}]", end="", flush=True)
                elif event.item.type == "tool_call_output_item":
                    print(f"\n   ‚úÖ [Tool completed]", end="", flush=True)
                    print("\nü§ñ **Assistant:** ", end="", flush=True)
        
        print()  # New line after response
        
        # Add assistant response to history
        self.conversation_history.append({
            "role": "assistant",
            "content": response_text,
            "timestamp": datetime.now().isoformat()
        })
        
        return response_text
    
    def handle_command(self, command: str) -> bool:
        """Handle special commands. Returns True if should continue chat."""
        
        if command == "/help":
            print("""
üìã **Available Commands:**
   ‚Ä¢ /help     - Show this help message
   ‚Ä¢ /history  - Show conversation history
   ‚Ä¢ /clear    - Clear conversation history
   ‚Ä¢ /stats    - Show chat statistics
   ‚Ä¢ /quit     - Exit the chat
            """)
        
        elif command == "/history":
            if not self.conversation_history:
                print("üìù No conversation history yet.")
            else:
                print("\nüìñ **Conversation History:**")
                for i, msg in enumerate(self.conversation_history, 1):
                    role_icon = "üë§" if msg["role"] == "user" else "ü§ñ"
                    timestamp = datetime.fromisoformat(msg["timestamp"]).strftime("%H:%M")
                    print(f"{i:2d}. {role_icon} [{timestamp}] {msg['content'][:80]}{'...' if len(msg['content']) > 80 else ''}")
        
        elif command == "/clear":
            self.conversation_history.clear()
            print("üóëÔ∏è Conversation history cleared.")
        
        elif command == "/stats":
            total_messages = len(self.conversation_history)
            user_messages = len([m for m in self.conversation_history if m["role"] == "user"])
            assistant_messages = len([m for m in self.conversation_history if m["role"] == "assistant"])
            
            print(f"""
üìä **Chat Statistics:**
   ‚Ä¢ Total messages: {total_messages}
   ‚Ä¢ Your messages: {user_messages}
   ‚Ä¢ Assistant messages: {assistant_messages}
   ‚Ä¢ Average response length: {sum(len(m['content']) for m in self.conversation_history if m['role'] == 'assistant') // max(assistant_messages, 1)} characters
            """)
        
        elif command == "/quit":
            print("üëã Thanks for chatting! Goodbye!")
            return False
        
        else:
            print(f"‚ùì Unknown command: {command}. Type '/help' for available commands.")
        
        return True
    
    async def run(self):
        """Main chat loop"""
        self.display_header()
        
        try:
            while True:
                user_input = self.get_user_input()
                
                if not user_input:
                    continue
                
                # Handle commands
                if user_input.startswith("/"):
                    should_continue = self.handle_command(user_input)
                    if not should_continue:
                        break
                    continue
                
                # Process regular message
                try:
                    await self.stream_agent_response(user_input)
                    
                except KeyboardInterrupt:
                    print("\n\n‚è∏Ô∏è Response interrupted by user.")
                    continue
                    
                except Exception as e:
                    print(f"\n‚ùå **Error:** {str(e)}")
                    print("üîÑ Please try again or type '/help' for assistance.")
        
        except Exception as e:
            print(f"\nüí• **Unexpected error:** {str(e)}")
        
        finally:
            print("\nüëã Chat session ended.")

# Demo the streaming chat interface
async def demo_streaming_chat():
    """Demo interactive streaming chat"""
    chat = StreamingChatInterface()
    await chat.run()

if __name__ == "__main__":
    asyncio.run(demo_streaming_chat())
```

### Web-based Streaming Interface

```python
from typing import AsyncGenerator, Dict, Any
import json
import asyncio
from datetime import datetime

class WebStreamingService:
    """Service for web-based streaming interfaces"""
    
    def __init__(self):
        self.active_sessions: Dict[str, Dict[str, Any]] = {}
        
        self.web_agent = Agent(
            name="Web Assistant",
            instructions="""
            B·∫°n l√† m·ªôt AI assistant ƒë∆∞·ª£c t·ªëi ∆∞u cho web interface.
            
            Web-specific behaviors:
            - Responses should be well-structured for HTML rendering
            - Use markdown formatting when appropriate
            - Be concise but informative
            - Include relevant emojis for visual appeal
            - Suggest follow-up actions when helpful
            
            Response format:
            - Short paragraphs for better readability
            - Use bullet points for lists
            - Bold important information
            - Include examples when explaining concepts
            """
        )
    
    async def create_session(self, session_id: str, user_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Create new streaming session"""
        
        self.active_sessions[session_id] = {
            "created_at": datetime.now().isoformat(),
            "user_context": user_context or {},
            "message_count": 0,
            "last_activity": datetime.now().isoformat()
        }
        
        return {
            "session_id": session_id,
            "status": "created",
            "timestamp": datetime.now().isoformat()
        }
    
    async def stream_response(
        self, 
        session_id: str, 
        user_message: str
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Stream response for web interface"""
        
        if session_id not in self.active_sessions:
            yield {
                "type": "error",
                "data": {
                    "message": "Session not found",
                    "code": "SESSION_NOT_FOUND"
                }
            }
            return
        
        # Update session activity
        session = self.active_sessions[session_id]
        session["last_activity"] = datetime.now().isoformat()
        session["message_count"] += 1
        
        # Yield session update
        yield {
            "type": "session_update",
            "data": {
                "session_id": session_id,
                "message_count": session["message_count"],
                "timestamp": datetime.now().isoformat()
            }
        }
        
        # Yield typing indicator
        yield {
            "type": "typing_start",
            "data": {
                "message": "Assistant is thinking...",
                "timestamp": datetime.now().isoformat()
            }
        }
        
        # Stream the actual response
        try:
            result = Runner.run_streamed(self.web_agent, user_message)
            
            response_buffer = ""
            word_count = 0
            
            async for event in result.stream_events():
                
                # Handle text streaming
                if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                    delta = event.data.delta
                    response_buffer += delta
                    
                    # Send periodic updates (every few words)
                    if " " in delta:
                        word_count += delta.count(" ")
                        
                        if word_count % 5 == 0:  # Every 5 words
                            yield {
                                "type": "text_delta",
                                "data": {
                                    "delta": delta,
                                    "full_text": response_buffer,
                                    "word_count": word_count,
                                    "timestamp": datetime.now().isoformat()
                                }
                            }
                    else:
                        yield {
                            "type": "text_delta", 
                            "data": {
                                "delta": delta,
                                "timestamp": datetime.now().isoformat()
                            }
                        }
                
                # Handle structured events
                elif event.type == "run_item_stream_event":
                    
                    if event.item.type == "tool_call_item":
                        yield {
                            "type": "tool_start",
                            "data": {
                                "tool_name": event.item.tool_name,
                                "message": f"Using {event.item.tool_name}...",
                                "timestamp": datetime.now().isoformat()
                            }
                        }
                    
                    elif event.item.type == "tool_call_output_item":
                        yield {
                            "type": "tool_complete",
                            "data": {
                                "tool_output": str(event.item.output)[:200] + "..." if len(str(event.item.output)) > 200 else str(event.item.output),
                                "message": "Tool execution completed",
                                "timestamp": datetime.now().isoformat()
                            }
                        }
            
            # Final response completion
            yield {
                "type": "response_complete",
                "data": {
                    "final_response": response_buffer,
                    "word_count": len(response_buffer.split()),
                    "character_count": len(response_buffer),
                    "session_id": session_id,
                    "timestamp": datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            yield {
                "type": "error",
                "data": {
                    "message": f"Stream error: {str(e)}",
                    "code": "STREAM_ERROR",
                    "timestamp": datetime.now().isoformat()
                }
            }
    
    def get_session_stats(self, session_id: str) -> Dict[str, Any]:
        """Get session statistics"""
        
        if session_id not in self.active_sessions:
            return {"error": "Session not found"}
        
        session = self.active_sessions[session_id]
        
        return {
            "session_id": session_id,
            "created_at": session["created_at"],
            "last_activity": session["last_activity"],
            "message_count": session["message_count"],
            "duration_minutes": (
                datetime.now() - datetime.fromisoformat(session["created_at"])
            ).total_seconds() / 60,
            "status": "active"
        }

# Demo web streaming service
async def demo_web_streaming():
    """Demo web streaming service"""
    
    print("üåê **Web Streaming Service Demo**\n")
    
    streaming_service = WebStreamingService()
    
    # Create session
    session_id = "web_session_123"
    session_info = await streaming_service.create_session(
        session_id, 
        {"user_agent": "Demo Browser", "ip": "127.0.0.1"}
    )
    
    print(f"üì± Session created: {session_info}")
    
    # Test messages
    test_messages = [
        "Gi·∫£i th√≠ch cho t√¥i v·ªÅ machine learning m·ªôt c√°ch ƒë∆°n gi·∫£n",
        "T√¥i c√≥ n√™n h·ªçc Python hay JavaScript tr∆∞·ªõc?",
        "L√†m th·∫ø n√†o ƒë·ªÉ tr·ªü th√†nh m·ªôt developer gi·ªèi?"
    ]
    
    for i, message in enumerate(test_messages, 1):
        print(f"\nüí¨ **Message {i}:** {message}")
        print("üì° **Streaming events:**")
        
        event_count = 0
        async for event in streaming_service.stream_response(session_id, message):
            event_count += 1
            
            if event["type"] == "typing_start":
                print(f"   ‚è≥ {event['data']['message']}")
            
            elif event["type"] == "text_delta":
                if "delta" in event["data"]:
                    print(event["data"]["delta"], end="", flush=True)
            
            elif event["type"] == "tool_start":
                print(f"\n   üîß {event['data']['message']}")
            
            elif event["type"] == "tool_complete":
                print(f"\n   ‚úÖ {event['data']['message']}")
            
            elif event["type"] == "response_complete":
                data = event["data"]
                print(f"\n\nüìä Response complete:")
                print(f"   ‚Ä¢ Words: {data['word_count']}")
                print(f"   ‚Ä¢ Characters: {data['character_count']}")
        
        print(f"\n   üìà Total events: {event_count}")
        print("=" * 60)
    
    # Show session stats
    stats = streaming_service.get_session_stats(session_id)
    print(f"\nüìà **Session Statistics:**")
    print(f"   ‚Ä¢ Duration: {stats['duration_minutes']:.1f} minutes")
    print(f"   ‚Ä¢ Messages: {stats['message_count']}")
    print(f"   ‚Ä¢ Status: {stats['status']}")

if __name__ == "__main__":
    from openai.types.responses import ResponseTextDeltaEvent
    asyncio.run(demo_web_streaming())
```

## Error Handling v√† Recovery

### Robust Streaming v·ªõi Error Recovery

```python
import asyncio
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
from agents import Agent, Runner
from agents.exceptions import AgentsException

class ResilientStreamingSystem:
    """Streaming system v·ªõi comprehensive error handling"""
    
    def __init__(self):
        self.retry_config = {
            "max_retries": 3,
            "base_delay": 1.0,
            "max_delay": 10.0,
            "backoff_multiplier": 2.0
        }
        
        self.error_statistics = {
            "total_errors": 0,
            "network_errors": 0,
            "timeout_errors": 0,
            "agent_errors": 0,
            "recovery_successes": 0
        }
        
        self.resilient_agent = Agent(
            name="Resilient Assistant",
            instructions="""
            B·∫°n l√† m·ªôt AI assistant ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ robust v√† reliable.
            
            Characteristics:
            - Always try to provide helpful responses
            - Gracefully handle interruptions
            - Acknowledge when recovering from errors
            - Be transparent about limitations
            - Maintain conversation context even after errors
            
            If you detect that a previous response was interrupted:
            - Acknowledge the interruption
            - Offer to continue or restart
            - Ask if the user needs clarification
            """
        )
    
    async def stream_with_recovery(
        self, 
        user_message: str,
        context: Optional[str] = None,
        timeout: float = 30.0
    ) -> Dict[str, Any]:
        """Stream response v·ªõi automatic error recovery"""
        
        attempt = 0
        last_error = None
        partial_response = ""
        
        while attempt < self.retry_config["max_retries"]:
            try:
                print(f"üîÑ Attempt {attempt + 1}/{self.retry_config['max_retries']}")
                
                # Build full context including any partial response
                full_context = self._build_recovery_context(user_message, context, partial_response, attempt)
                
                # Stream v·ªõi timeout
                result = await asyncio.wait_for(
                    self._stream_response(full_context, partial_response),
                    timeout=timeout
                )
                
                # Success!
                if attempt > 0:
                    self.error_statistics["recovery_successes"] += 1
                    print(f"‚úÖ Recovered successfully after {attempt} attempts")
                
                return {
                    "success": True,
                    "response": result["response"],
                    "attempts": attempt + 1,
                    "recovered": attempt > 0,
                    "partial_content": partial_response
                }
                
            except asyncio.TimeoutError as e:
                self.error_statistics["timeout_errors"] += 1
                last_error = f"Timeout after {timeout}s"
                print(f"‚è∞ Timeout error: {last_error}")
                
            except AgentsException as e:
                self.error_statistics["agent_errors"] += 1
                last_error = f"Agent error: {str(e)}"
                print(f"ü§ñ Agent error: {last_error}")
                
            except Exception as e:
                self.error_statistics["network_errors"] += 1
                last_error = f"Network error: {str(e)}"
                print(f"üåê Network error: {last_error}")
            
            attempt += 1
            self.error_statistics["total_errors"] += 1
            
            if attempt < self.retry_config["max_retries"]:
                # Calculate delay v·ªõi exponential backoff
                delay = min(
                    self.retry_config["base_delay"] * (self.retry_config["backoff_multiplier"] ** attempt),
                    self.retry_config["max_delay"]
                )
                
                print(f"‚è≥ Retrying in {delay:.1f} seconds...")
                await asyncio.sleep(delay)
        
        # All retries failed
        print(f"‚ùå All {self.retry_config['max_retries']} attempts failed")
        
        return {
            "success": False,
            "error": last_error,
            "attempts": attempt,
            "partial_content": partial_response,
            "recovery_failed": True
        }
    
    def _build_recovery_context(
        self, 
        original_message: str, 
        context: Optional[str], 
        partial_response: str, 
        attempt: int
    ) -> str:
        """Build context cho recovery attempts"""
        
        recovery_context = f"User message: {original_message}\n"
        
        if context:
            recovery_context += f"Additional context: {context}\n"
        
        if attempt > 0:
            recovery_context += f"\nNote: This is attempt #{attempt + 1} due to previous errors."
            
            if partial_response:
                recovery_context += f"\nPartial response was generated: {partial_response[:200]}..."
                recovery_context += "\nPlease continue from where it left off or provide a complete response."
            else:
                recovery_context += "\nPrevious attempt failed before generating response. Please try again."
        
        return recovery_context
    
    async def _stream_response(self, message: str, existing_partial: str = "") -> Dict[str, Any]:
        """Internal streaming method"""
        
        result = Runner.run_streamed(self.resilient_agent, message)
        
        response_text = existing_partial
        events_processed = 0
        
        async for event in result.stream_events():
            events_processed += 1
            
            if event.type == "raw_response_event":
                from openai.types.responses import ResponseTextDeltaEvent
                if isinstance(event.data, ResponseTextDeltaEvent):
                    delta = event.data.delta
                    response_text += delta
                    print(delta, end="", flush=True)
            
            # Simulate random errors for testing
            if events_processed > 10 and random.random() < 0.1:  # 10% chance
                raise Exception("Simulated network interruption")
        
        print()  # New line after response
        
        return {
            "response": response_text,
            "events_processed": events_processed
        }
    
    def get_error_statistics(self) -> Dict[str, Any]:
        """Get comprehensive error statistics"""
        
        total_errors = self.error_statistics["total_errors"]
        
        if total_errors == 0:
            error_rate = 0.0
            recovery_rate = 0.0
        else:
            error_rate = total_errors / (total_errors + self.error_statistics["recovery_successes"])
            recovery_rate = self.error_statistics["recovery_successes"] / total_errors
        
        return {
            **self.error_statistics,
            "error_rate": error_rate,
            "recovery_rate": recovery_rate,
            "last_updated": datetime.now().isoformat()
        }

# Demo resilient streaming
async def demo_resilient_streaming():
    """Demo resilient streaming system"""
    
    print("üõ°Ô∏è **Resilient Streaming System Demo**\n")
    
    streaming_system = ResilientStreamingSystem()
    
    test_scenarios = [
        {
            "message": "Gi·∫£i th√≠ch v·ªÅ quantum computing m·ªôt c√°ch ƒë∆°n gi·∫£n",
            "context": "User is a beginner in technology"
        },
        {
            "message": "T·∫°o m·ªôt plan h·ªçc l·∫≠p tr√¨nh trong 6 th√°ng",
            "context": "User c√≥ background v·ªÅ to√°n h·ªçc"
        },
        {
            "message": "So s√°nh c√°c framework JavaScript ph·ªï bi·∫øn",
            "context": "User ƒëang t√¨m hi·ªÉu v·ªÅ web development"
        }
    ]
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"üí¨ **Scenario {i}:** {scenario['message']}")
        print(f"üìù **Context:** {scenario['context']}")
        print("ü§ñ **Response:**")
        
        result = await streaming_system.stream_with_recovery(
            scenario["message"],
            scenario["context"],
            timeout=20.0
        )
        
        if result["success"]:
            print(f"\n‚úÖ Success (attempts: {result['attempts']})")
            if result["recovered"]:
                print("üîÑ Response was recovered from error")
        else:
            print(f"\n‚ùå Failed: {result['error']}")
            if result.get("partial_content"):
                print(f"üìù Partial content: {result['partial_content'][:100]}...")
        
        print("=" * 70 + "\n")
    
    # Show error statistics
    stats = streaming_system.get_error_statistics()
    print("üìä **Error Statistics:**")
    print(f"   ‚Ä¢ Total errors: {stats['total_errors']}")
    print(f"   ‚Ä¢ Network errors: {stats['network_errors']}")
    print(f"   ‚Ä¢ Timeout errors: {stats['timeout_errors']}")  
    print(f"   ‚Ä¢ Agent errors: {stats['agent_errors']}")
    print(f"   ‚Ä¢ Recovery successes: {stats['recovery_successes']}")
    print(f"   ‚Ä¢ Recovery rate: {stats['recovery_rate']:.1%}")

if __name__ == "__main__":
    import random
    asyncio.run(demo_resilient_streaming())
```

## Performance Optimization

### Streaming Performance Monitoring

```python
import time
import statistics
from typing import List, Dict, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta

@dataclass
class StreamingMetrics:
    """Metrics for streaming performance"""
    session_id: str
    start_time: float = field(default_factory=time.time)
    first_token_time: Optional[float] = None
    completion_time: Optional[float] = None
    total_tokens: int = 0
    total_events: int = 0
    error_count: int = 0
    interruption_count: int = 0
    
    def calculate_latencies(self) -> Dict[str, float]:
        """Calculate various latency metrics"""
        current_time = time.time()
        
        return {
            "time_to_first_token": self.first_token_time - self.start_time if self.first_token_time else None,
            "total_response_time": self.completion_time - self.start_time if self.completion_time else current_time - self.start_time,
            "tokens_per_second": self.total_tokens / (current_time - self.start_time) if current_time > self.start_time else 0,
            "events_per_second": self.total_events / (current_time - self.start_time) if current_time > self.start_time else 0
        }

class StreamingPerformanceMonitor:
    """Monitor v√† optimize streaming performance"""
    
    def __init__(self):
        self.active_sessions: Dict[str, StreamingMetrics] = {}
        self.completed_sessions: List[StreamingMetrics] = []
        self.performance_thresholds = {
            "max_time_to_first_token": 2.0,  # seconds
            "min_tokens_per_second": 10,     # tokens/sec
            "max_error_rate": 0.05           # 5%
        }
    
    def start_session(self, session_id: str) -> StreamingMetrics:
        """Start monitoring a new streaming session"""
        
        metrics = StreamingMetrics(session_id=session_id)
        self.active_sessions[session_id] = metrics
        
        return metrics
    
    def record_first_token(self, session_id: str):
        """Record when first token was received"""
        if session_id in self.active_sessions:
            self.active_sessions[session_id].first_token_time = time.time()
    
    def record_token(self, session_id: str, token: str):
        """Record a new token"""
        if session_id in self.active_sessions:
            self.active_sessions[session_id].total_tokens += 1
    
    def record_event(self, session_id: str, event_type: str):
        """Record a streaming event"""
        if session_id in self.active_sessions:
            metrics = self.active_sessions[session_id]
            metrics.total_events += 1
            
            if event_type == "error":
                metrics.error_count += 1
            elif event_type == "interruption":
                metrics.interruption_count += 1
    
    def complete_session(self, session_id: str) -> Dict[str, Any]:
        """Complete a streaming session v√† calculate final metrics"""
        
        if session_id not in self.active_sessions:
            return {"error": "Session not found"}
        
        metrics = self.active_sessions[session_id]
        metrics.completion_time = time.time()
        
        # Calculate final metrics
        latencies = metrics.calculate_latencies()
        
        # Move to completed sessions
        self.completed_sessions.append(metrics)
        del self.active_sessions[session_id]
        
        # Performance assessment
        performance_issues = self._assess_performance(metrics, latencies)
        
        return {
            "session_id": session_id,
            "metrics": metrics,
            "latencies": latencies,
            "performance_issues": performance_issues,
            "status": "completed"
        }
    
    def _assess_performance(self, metrics: StreamingMetrics, latencies: Dict[str, float]) -> List[str]:
        """Assess performance v√† identify issues"""
        
        issues = []
        
        # Check time to first token
        if latencies["time_to_first_token"] and latencies["time_to_first_token"] > self.performance_thresholds["max_time_to_first_token"]:
            issues.append(f"Slow first token: {latencies['time_to_first_token']:.2f}s > {self.performance_thresholds['max_time_to_first_token']}s")
        
        # Check tokens per second
        if latencies["tokens_per_second"] < self.performance_thresholds["min_tokens_per_second"]:
            issues.append(f"Low throughput: {latencies['tokens_per_second']:.1f} tokens/s < {self.performance_thresholds['min_tokens_per_second']}")
        
        # Check error rate
        if metrics.total_events > 0:
            error_rate = metrics.error_count / metrics.total_events
            if error_rate > self.performance_thresholds["max_error_rate"]:
                issues.append(f"High error rate: {error_rate:.1%} > {self.performance_thresholds['max_error_rate']:.1%}")
        
        return issues
    
    def get_global_stats(self) -> Dict[str, Any]:
        """Get global performance statistics"""
        
        if not self.completed_sessions:
            return {"message": "No completed sessions yet"}
        
        # Aggregate metrics
        all_sessions = self.completed_sessions
        
        ttft_times = [s.first_token_time - s.start_time for s in all_sessions if s.first_token_time]
        total_times = [s.completion_time - s.start_time for s in all_sessions if s.completion_time]
        token_rates = [s.total_tokens / (s.completion_time - s.start_time) for s in all_sessions if s.completion_time and s.completion_time > s.start_time]
        
        return {
            "total_sessions": len(all_sessions),
            "average_time_to_first_token": statistics.mean(ttft_times) if ttft_times else 0,
            "median_time_to_first_token": statistics.median(ttft_times) if ttft_times else 0,
            "average_total_time": statistics.mean(total_times) if total_times else 0,
            "average_tokens_per_second": statistics.mean(token_rates) if token_rates else 0,
            "total_tokens_processed": sum(s.total_tokens for s in all_sessions),
            "total_errors": sum(s.error_count for s in all_sessions),
            "sessions_with_issues": len([s for s in all_sessions if s.error_count > 0 or s.interruption_count > 0]),
            "last_updated": datetime.now().isoformat()
        }

# Optimized streaming agent v·ªõi performance monitoring
class OptimizedStreamingAgent:
    """High-performance streaming agent v·ªõi monitoring"""
    
    def __init__(self):
        self.performance_monitor = StreamingPerformanceMonitor()
        
        self.optimized_agent = Agent(
            name="High Performance Assistant",
            instructions="""
            B·∫°n l√† m·ªôt AI assistant ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho performance.
            
            Performance characteristics:
            - Respond quickly and efficiently
            - Use concise but informative language
            - Structure responses for optimal streaming
            - Minimize unnecessary processing
            
            Response strategy:
            - Start with direct answer
            - Add details progressively
            - Use short sentences for better streaming flow
            - Include examples when helpful but keep them brief
            """
        )
    
    async def optimized_stream_response(self, session_id: str, user_message: str) -> Dict[str, Any]:
        """Stream response v·ªõi performance optimization"""
        
        # Start monitoring
        metrics = self.performance_monitor.start_session(session_id)
        
        try:
            # Stream response
            result = Runner.run_streamed(self.optimized_agent, user_message)
            
            response_text = ""
            first_token_recorded = False
            
            async for event in result.stream_events():
                
                # Record events
                self.performance_monitor.record_event(session_id, event.type)
                
                if event.type == "raw_response_event":
                    from openai.types.responses import ResponseTextDeltaEvent
                    if isinstance(event.data, ResponseTextDeltaEvent):
                        
                        # Record first token
                        if not first_token_recorded:
                            self.performance_monitor.record_first_token(session_id)
                            first_token_recorded = True
                        
                        # Record token
                        delta = event.data.delta
                        self.performance_monitor.record_token(session_id, delta)
                        response_text += delta
                        
                        # Stream to output
                        print(delta, end="", flush=True)
                
                elif event.type == "run_item_stream_event":
                    # Handle tool calls efficiently  
                    if event.item.type == "tool_call_item":
                        print(f"\nüîß [{event.item.tool_name}]", end="", flush=True)
            
            print()  # New line after completion
            
            # Complete session monitoring
            session_result = self.performance_monitor.complete_session(session_id)
            
            return {
                "success": True,
                "response": response_text,
                "performance": session_result
            }
            
        except Exception as e:
            self.performance_monitor.record_event(session_id, "error")
            session_result = self.performance_monitor.complete_session(session_id)
            
            return {
                "success": False,
                "error": str(e),
                "performance": session_result
            }

# Demo optimized streaming
async def demo_optimized_streaming():
    """Demo optimized streaming v·ªõi performance monitoring"""
    
    print("‚ö° **Optimized Streaming Performance Demo**\n")
    
    streaming_agent = OptimizedStreamingAgent()
    
    # Test different types of queries
    test_queries = [
        "Gi·∫£i th√≠ch nhanh v·ªÅ React hooks",
        "So s√°nh Python v√† JavaScript cho beginners",
        "T·∫°o m·ªôt TODO list ƒë∆°n gi·∫£n b·∫±ng HTML/CSS/JS",
        "Explain machine learning trong 2 ph√∫t",
        "Best practices cho database design"
    ]
    
    for i, query in enumerate(test_queries, 1):
        session_id = f"perf_session_{i}"
        
        print(f"üöÄ **Query {i}:** {query}")
        print("üì° **Streaming response:**")
        
        start_time = time.time()
        result = await streaming_agent.optimized_stream_response(session_id, query)
        end_time = time.time()
        
        if result["success"]:
            perf = result["performance"]
            latencies = perf["latencies"]
            
            print(f"\nüìä **Performance Metrics:**")
            print(f"   ‚Ä¢ Time to First Token: {latencies['time_to_first_token']:.3f}s")
            print(f"   ‚Ä¢ Total Response Time: {latencies['total_response_time']:.3f}s")
            print(f"   ‚Ä¢ Tokens per Second: {latencies['tokens_per_second']:.1f}")
            print(f"   ‚Ä¢ Total Tokens: {perf['metrics'].total_tokens}")
            
            if perf["performance_issues"]:
                print(f"   ‚ö†Ô∏è Issues: {', '.join(perf['performance_issues'])}")
            else:
                print(f"   ‚úÖ Performance: Good")
        
        else:
            print(f"\n‚ùå Error: {result['error']}")
        
        print("=" * 70 + "\n")
    
    # Show global performance stats
    global_stats = streaming_agent.performance_monitor.get_global_stats()
    
    print("üìà **Global Performance Statistics:**")
    print(f"   ‚Ä¢ Total Sessions: {global_stats['total_sessions']}")
    print(f"   ‚Ä¢ Avg Time to First Token: {global_stats['average_time_to_first_token']:.3f}s")
    print(f"   ‚Ä¢ Avg Tokens/Second: {global_stats['average_tokens_per_second']:.1f}")
    print(f"   ‚Ä¢ Total Tokens Processed: {global_stats['total_tokens_processed']}")
    print(f"   ‚Ä¢ Sessions with Issues: {global_stats['sessions_with_issues']}")

if __name__ == "__main__":
    asyncio.run(demo_optimized_streaming())
```

## T·ªïng K·∫øt v√† Best Practices

### Nh·ªØng ƒëi·ªÉm ch√≠nh c·∫ßn nh·ªõ

‚úÖ **∆Øu ti√™n tr·∫£i nghi·ªám ng∆∞·ªùi d√πng** - Streaming c·∫£i thi·ªán ƒë√°ng k·ªÉ c·∫£m gi√°c ph·∫£n h·ªìi nhanh v√† t·ª± nhi√™n
‚úÖ **Hai lo·∫°i s·ª± ki·ªán** - Raw Response Events cho t·ª´ng t·ª´ hi·ªÉn th·ªã, Run Item Events cho th√¥ng tin c·∫•u tr√∫c
‚úÖ **X·ª≠ l√Ω l·ªói th√¥ng minh** - Ch·ªß ƒë·ªông x·ª≠ l√Ω gi√°n ƒëo·∫°n m·∫°ng v√† l·ªói h·ªá th·ªëng
‚úÖ **Gi√°m s√°t hi·ªáu su·∫•t** - ƒêo l∆∞·ªùng c√°c ch·ªâ s·ªë streaming ƒë·ªÉ t·ªëi ∆∞u tr·∫£i nghi·ªám
‚úÖ **Giao di·ªán ph·∫£n h·ªìi nhanh** - Th√≠ch h·ª£p cho c·∫£ ·ª©ng d·ª•ng console v√† web

### Nh·ªØng best practice khi tri·ªÉn khai Streaming

üéØ **V·ªÅ tr·∫£i nghi·ªám ng∆∞·ªùi d√πng:**

* Lu√¥n hi·ªÉn th·ªã ph·∫£n h·ªìi ngay l·∫≠p t·ª©c khi b·∫Øt ƒë·∫ßu x·ª≠ l√Ω
* S·ª≠ d·ª•ng c√°c ch·ªâ b√°o tr·∫°ng th√°i v√† ti·∫øn ƒë·ªô r√µ r√†ng
* Cho ph√©p ng∆∞·ªùi d√πng ng·∫Øt c√°c ph·∫£n h·ªìi qu√° d√†i
* Lu√¥n cung c·∫•p ng·ªØ c·∫£nh r√µ r√†ng v·ªÅ nh·ªØng g√¨ ƒëang di·ªÖn ra
* X·ª≠ l√Ω l·ªói m·ªôt c√°ch m∆∞·ª£t m√† v√† ƒë·ªÅ xu·∫•t c√°c ph∆∞∆°ng √°n kh·∫Øc ph·ª•c

‚ö° **V·ªÅ hi·ªáu su·∫•t:**

* Theo d√µi s√°t sao ƒë·ªô tr·ªÖ c·ªßa token ƒë·∫ßu ti√™n (time-to-first-token)
* ƒê·∫£m b·∫£o t·ªëc ƒë·ªô token ·ªïn ƒë·ªãnh v√† nhanh ch√≥ng
* Th·ª±c hi·ªán buffering m·ªôt c√°ch th√¥ng minh v√† hi·ªáu qu·∫£
* L∆∞u cache nh·ªØng ph·∫£n h·ªìi th∆∞·ªùng xuy√™n l·∫∑p l·∫°i
* S·ª≠ d·ª•ng k·ªπ thu·∫≠t connection pooling ƒë·ªÉ duy tr√¨ hi·ªáu su·∫•t

üîß **V·ªÅ m·∫∑t k·ªπ thu·∫≠t:**

* X·ª≠ l√Ω ƒë·ªìng th·ªùi c√°c s·ª± ki·ªán streaming ·ªü c·∫•p ƒë·ªô raw v√† c·∫•u tr√∫c
* √Åp d·ª•ng c√°c c∆° ch·∫ø kh√¥i ph·ª•c l·ªói to√†n di·ªán v√† m·∫°nh m·∫Ω
* Tri·ªÉn khai gi√°m s√°t hi·ªáu su·∫•t ngay t·ª´ ban ƒë·∫ßu
* Thi·∫øt k·∫ø ph√π h·ª£p cho c·∫£ giao di·ªán console v√† web
* Ki·ªÉm th·ª≠ k·ªπ l∆∞·ª°ng trong nhi·ªÅu ƒëi·ªÅu ki·ªán m·∫°ng kh√°c nhau

### H∆∞·ªõng d·∫´n tri·ªÉn khai cho h·ªá th·ªëng production

üìä **Thi·∫øt l·∫≠p gi√°m s√°t:**

* Theo d√µi ƒë·ªô tr·ªÖ token ƒë·∫ßu ti√™n cho m·ªçi y√™u c·∫ßu
* Ki·ªÉm so√°t t·ªëc ƒë·ªô x·ª≠ l√Ω token v√† t·ª∑ l·ªá l·ªói
* Thi·∫øt l·∫≠p c·∫£nh b√°o khi hi·ªáu su·∫•t gi·∫£m s√∫t
* Ghi log c√°c tr∆∞·ªùng h·ª£p gi√°n ƒëo·∫°n streaming v√† k·∫øt qu·∫£ kh√¥i ph·ª•c
* ƒê√°nh gi√° t∆∞∆°ng t√°c c·ªßa ng∆∞·ªùi d√πng khi d√πng streaming so v·ªõi c√°ch truy·ªÅn th·ªëng

üõ°Ô∏è **X·ª≠ l√Ω l·ªói:**

* Th·ª±c hi·ªán retry v·ªõi chi·∫øn l∆∞·ª£c backoff tƒÉng d·∫ßn
* B·∫£o to√†n c√°c ph·∫£n h·ªìi m·ªôt ph·∫ßn khi g·∫∑p l·ªói
* Cung c·∫•p th√¥ng b√°o l·ªói th√¢n thi·ªán v√† d·ªÖ hi·ªÉu cho ng∆∞·ªùi d√πng
* Cho ph√©p ng∆∞·ªùi d√πng th·ª≠ l·∫°i m·ªôt c√°ch ch·ªß ƒë·ªông
* Theo d√µi c√°c m√¥ h√¨nh l·ªói ƒë·ªÉ ph√°t hi·ªán v·∫•n ƒë·ªÅ h·ªá th·ªëng

üîÑ **C√°c chi·∫øn l∆∞·ª£c t·ªëi ∆∞u:**

* Duy tr√¨ s·∫µn k·∫øt n·ªëi ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ ban ƒë·∫ßu
* Th·ª±c hi·ªán caching th√¥ng minh cho c√°c truy v·∫•n ph·ªï bi·∫øn
* √Åp d·ª•ng k·∫øt n·ªëi ƒëa lu·ªìng khi c√≥ th·ªÉ
* T·ªëi ∆∞u h√≥a c√°c thi·∫øt l·∫≠p m√¥ h√¨nh AI ƒë·ªÉ ph√π h·ª£p v·ªõi streaming
* C√¢n nh·∫Øc tri·ªÉn khai ·ªü c√°c edge server ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ m·∫°ng

### Nh·ªØng l·ªói th∆∞·ªùng g·∫∑p v√† c√°ch kh·∫Øc ph·ª•c

‚ùå **B·ªè qua ƒë·ªô tr·ªÖ token ƒë·∫ßu ti√™n** ‚Üí Lu√¥n gi√°m s√°t v√† t·ªëi ∆∞u ƒë·ªô tr·ªÖ ban ƒë·∫ßu
‚ùå **Kh√¥ng x·ª≠ l√Ω l·ªói** ‚Üí Th·ª±c hi·ªán retry m·∫°nh m·∫Ω v√† th√¥ng minh
‚ùå **Kh√¥ng c√≥ ch·ªâ b√°o ti·∫øn ƒë·ªô r√µ r√†ng** ‚Üí Lu√¥n hi·ªÉn th·ªã tr·∫°ng th√°i x·ª≠ l√Ω r√µ r√†ng
‚ùå **Hi·ªÉn th·ªã qu√° nhi·ªÅu th√¥ng tin** ‚Üí C√¢n b·∫±ng gi·ªØa th√¥ng tin cung c·∫•p v√† tr·∫£i nghi·ªám ng∆∞·ªùi d√πng
‚ùå **Kh√¥ng gi√°m s√°t hi·ªáu su·∫•t** ‚Üí Theo d√µi c√°c ch·ªâ s·ªë hi·ªáu su·∫•t ngay t·ª´ ƒë·∫ßu

### C√°c m√¥ h√¨nh streaming n√¢ng cao

üé® **C·∫£i ti·∫øn v·ªÅ giao di·ªán v√† tr·∫£i nghi·ªám:**

* Progressive disclosure - hi·ªÉn th·ªã t√≥m t·∫Øt tr∆∞·ªõc, chi ti·∫øt sau
* Smart interruption - cho ph√©p ng∆∞·ªùi d√πng ƒë·∫∑t c√¢u h·ªèi th√™m trong qu√° tr√¨nh streaming
* B·∫£o to√†n ng·ªØ c·∫£nh - duy tr√¨ tr·∫°ng th√°i tr√≤ chuy·ªán d√π x·∫£y ra l·ªói
* Adaptive streaming - ƒëi·ªÅu ch·ªânh t·ªëc ƒë·ªô hi·ªÉn th·ªã theo kh·∫£ nƒÉng ƒë·ªçc c·ªßa ng∆∞·ªùi d√πng
* Ph·∫£n h·ªìi ƒëa ph∆∞∆°ng ti·ªán - k·∫øt h·ª£p vƒÉn b·∫£n, h√¨nh ·∫£nh v√† c√°c y·∫øu t·ªë t∆∞∆°ng t√°c

üîß **C√°c k·ªπ thu·∫≠t t·ªëi ∆∞u chuy√™n s√¢u:**

* Response chunking - chia nh·ªè c√°c ph·∫£n h·ªìi d√†i th√†nh c√°c ph·∫ßn logic
* X·ª≠ l√Ω song song - x·ª≠ l√Ω ƒë·ªìng th·ªùi nhi·ªÅu lu·ªìng streaming
* Buffering th√¥ng minh - t·ªëi ∆∞u h√≥a vi·ªác s·ª≠ d·ª•ng m·∫°ng
* N√©n d·ªØ li·ªáu - gi·∫£m bƒÉng th√¥ng cho c√°c ph·∫£n h·ªìi l·ªõn
* T√≠ch h·ª£p CDN - ph√¢n ph·ªëi t·∫£i streaming tr√™n c√°c khu v·ª±c ƒë·ªãa l√Ω

### B∆∞·ªõc ti·∫øp theo

Trong **b√†i ti·∫øp theo**, ch√∫ng ta s·∫Ω kh√°m ph√°:

üïµÔ∏è **Tracing v√† Debugging** - Gi√°m s√°t hi·ªáu nƒÉng v√† h√†nh vi c·ªßa Agent
üìä **C√°c m√¥ h√¨nh Observability** - Gi√°m s√°t to√†n di·ªán trong h·ªá th·ªëng production
üîç **Ph√¢n t√≠ch hi·ªáu su·∫•t** - ƒêi s√¢u v√†o c√°c k·ªπ thu·∫≠t t·ªëi ∆∞u h√≥a

### Th·ª≠ th√°ch d√†nh cho b·∫°n

1. **X√¢y d·ª±ng giao di·ªán chat ph·∫£n h·ªìi nhanh** v·ªõi h·ªó tr·ª£ streaming tr√™n web ho·∫∑c mobile
2. **Tri·ªÉn khai x·ª≠ l√Ω l·ªói th√¥ng minh** b·∫£o to√†n ng·ªØ c·∫£nh h·ªôi tho·∫°i
3. **T·∫°o dashboard theo d√µi hi·ªáu su·∫•t** c·ªßa streaming
4. **T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t streaming** cho t·ª´ng tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng c·ª• th·ªÉ
5. **Thi·∫øt k·∫ø adaptive streaming** ƒëi·ªÅu ch·ªânh theo h√†nh vi ng∆∞·ªùi d√πng

---

*B√†i ti·∫øp theo: [**"Tracing v√† Debugging: Gi√°m S√°t Hi·ªáu NƒÉng Agent"**](../openai-agents-sdk-part-09-tracing-debugging-monitoring/) - Ch√∫ng ta s·∫Ω h·ªçc c√°ch gi√°m s√°t, debug v√† t·ªëi ∆∞u hi·ªáu nƒÉng c·ªßa Agent v·ªõi c√°c c√¥ng c·ª• observability to√†n di·ªán.*
