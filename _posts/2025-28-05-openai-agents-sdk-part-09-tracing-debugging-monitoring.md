---
title: "Tracing vÃ  Debugging: Monitoring Agent Performance"
date: 2025-05-24 09:00:00 +0700
categories: [AI, OpenAI]
tags: [openai, agents, tracing, debugging, monitoring, observability]
author: toantranct
description: HÆ°á»›ng dáº«n chi tiáº¿t vá» Tracing vÃ  Debugging trong OpenAI Agents SDK. Tá»« built-in tracing Ä‘áº¿n custom processors - giÃ¡m sÃ¡t vÃ  tá»‘i Æ°u hiá»‡u suáº¥t agents trong production.
---

Khi phÃ¡t triá»ƒn AI agents, viá»‡c hiá»ƒu Ä‘Æ°á»£c chuyá»‡n gÃ¬ Ä‘ang diá»…n ra "bÃªn trong" agent lÃ  vÃ´ cÃ¹ng quan trá»ng. Táº¡i sao agent láº¡i chá»n tool nÃ y thay vÃ¬ tool khÃ¡c? Handoff xáº£y ra nhÆ° tháº¿ nÃ o? Performance cÃ³ bá»‹ bottleneck á»Ÿ Ä‘Ã¢u khÃ´ng? **Tracing vÃ  Debugging** chÃ­nh lÃ  chÃ¬a khÃ³a Ä‘á»ƒ tráº£ lá»i nhá»¯ng cÃ¢u há»i nÃ y.

Tracing khÃ´ng chá»‰ lÃ  viá»‡c "ghi log" Ä‘Æ¡n thuáº§n mÃ  lÃ  nghá»‡ thuáº­t **quan sÃ¡t vÃ  hiá»ƒu rÃµ hÃ nh vi** cá»§a agent systems. Giá»‘ng nhÆ° má»™t bÃ¡c sÄ© dÃ¹ng X-ray Ä‘á»ƒ nhÃ¬n vÃ o cÆ¡ thá»ƒ, tracing giÃºp chÃºng ta nhÃ¬n vÃ o "tÃ¢m trÃ­" cá»§a agents Ä‘á»ƒ tá»‘i Æ°u hÃ³a vÃ  cáº£i thiá»‡n performance.

HÃ´m nay chÃºng ta sáº½ khÃ¡m phÃ¡ cÃ¡ch biáº¿n agents tá»« nhá»¯ng "há»™p Ä‘en bÃ­ áº©n" thÃ nh nhá»¯ng "há»‡ thá»‘ng trong suá»‘t" mÃ  báº¡n cÃ³ thá»ƒ hiá»ƒu, giÃ¡m sÃ¡t vÃ  tá»‘i Æ°u hÃ³a má»™t cÃ¡ch hiá»‡u quáº£.

## Tracing LÃ  GÃ¬ VÃ  Táº¡i Sao Quan Trá»ng?

### KhÃ¡i Niá»‡m CÆ¡ Báº£n

**Tracing** trong OpenAI Agents SDK lÃ  há»‡ thá»‘ng thu tháº­p vÃ  ghi láº¡i táº¥t cáº£ hoáº¡t Ä‘á»™ng cá»§a agents - tá»« LLM calls, tool executions, handoffs, cho Ä‘áº¿n guardrails checks. Má»—i hoáº¡t Ä‘á»™ng Ä‘Æ°á»£c ghi thÃ nh **spans**, vÃ  táº­p há»£p cÃ¡c spans táº¡o thÃ nh má»™t **trace** hoÃ n chá»‰nh.

```python
# VÃ­ dá»¥ má»™t trace Ä‘Æ¡n giáº£n
Trace: "Customer Support Query"
â”œâ”€â”€ Span: Agent Run (Triage Agent) - 2.3s
â”‚   â”œâ”€â”€ Span: LLM Generation - 1.2s  
â”‚   â”œâ”€â”€ Span: Tool Call (search_knowledge_base) - 0.8s
â”‚   â””â”€â”€ Span: Handoff to Billing Agent - 0.3s
â””â”€â”€ Span: Agent Run (Billing Agent) - 1.7s
    â”œâ”€â”€ Span: LLM Generation - 0.9s
    â””â”€â”€ Span: Tool Call (check_account_status) - 0.8s
```

### Táº¡i Sao Cáº§n Tracing?

ðŸ” **Visibility** - Biáº¿t agents Ä‘ang lÃ m gÃ¬ vÃ  táº¡i sao  
ðŸ› **Debugging** - TÃ¬m ra nguyÃªn nhÃ¢n cá»§a bugs vÃ  unexpected behaviors  
âš¡ **Performance** - Identify bottlenecks vÃ  optimize response times  
ðŸ“Š **Analytics** - Hiá»ƒu usage patterns vÃ  user behaviors  
ðŸ›¡ï¸ **Monitoring** - Detect issues trÆ°á»›c khi chÃºng affect users  

### Built-in Tracing Architecture

OpenAI Agents SDK cÃ³ tracing Ä‘Æ°á»£c enable máº·c Ä‘á»‹nh vá»›i architecture nhÆ° sau:

```python
from agents import Agent, Runner, trace, enable_verbose_stdout_logging

# Enable detailed logging Ä‘á»ƒ xem tracing info
enable_verbose_stdout_logging()

async def demo_built_in_tracing():
    """Demo built-in tracing capabilities"""
    
    # Trace sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c táº¡o cho má»—i Runner.run() call
    agent = Agent(
        name="Demo Agent",
        instructions="You are a helpful assistant for demonstration purposes."
    )
    
    print("ðŸ” **Built-in Tracing Demo**")
    print("Tracing is automatically enabled for all agent runs.\n")
    
    # Simple agent run - tracing happens automatically
    result = await Runner.run(agent, "Explain what tracing is in simple terms")
    
    print(f"\nðŸ“Š **Trace Info:**")
    print(f"â€¢ Raw Responses: {len(result.raw_responses)}")
    print(f"â€¢ New Items: {len(result.new_items)}")
    print(f"â€¢ Final Output Length: {len(result.final_output)} characters")
    
    return result

if __name__ == "__main__":
    import asyncio
    asyncio.run(demo_built_in_tracing())
```

## Custom Tracing Processors

### Táº¡o Custom Trace Processor

Äá»ƒ cÃ³ control tá»‘t hÆ¡n vá» tracing data, báº¡n cÃ³ thá»ƒ táº¡o custom trace processors:

```python
import json
import time
from typing import Dict, Any, List
from datetime import datetime
from agents.tracing import TraceProcessor, Trace, Span
from agents import Agent, Runner, add_trace_processor

class DetailedTraceProcessor(TraceProcessor):
    """Custom trace processor Ä‘á»ƒ collect detailed analytics"""
    
    def __init__(self):
        self.traces: List[Dict[str, Any]] = []
        self.performance_metrics: Dict[str, List[float]] = {}
        self.error_logs: List[Dict[str, Any]] = []
    
    def process_trace(self, trace: Trace):
        """Process completed trace"""
        
        trace_data = {
            "trace_id": trace.trace_id,
            "workflow_name": trace.workflow_name or "Unknown",
            "started_at": trace.started_at.isoformat() if trace.started_at else None,
            "ended_at": trace.ended_at.isoformat() if trace.ended_at else None,
            "duration_ms": self._calculate_duration(trace),
            "spans": [],
            "metadata": trace.metadata or {}
        }
        
        # Process all spans trong trace
        for span in trace.spans:
            span_data = self._process_span(span)
            trace_data["spans"].append(span_data)
        
        # Store trace
        self.traces.append(trace_data)
        
        # Update performance metrics
        self._update_metrics(trace_data)
        
        # Log trace summary
        self._log_trace_summary(trace_data)
    
    def _process_span(self, span: Span) -> Dict[str, Any]:
        """Process individual span"""
        
        span_data = {
            "span_id": span.span_id,
            "parent_id": span.parent_id,
            "operation_name": span.operation_name,
            "started_at": span.started_at.isoformat() if span.started_at else None,
            "ended_at": span.ended_at.isoformat() if span.ended_at else None,
            "duration_ms": self._calculate_span_duration(span),
            "tags": span.tags or {},
            "logs": span.logs or []
        }
        
        # Extract specific information based on span type
        if span.operation_name == "agent_run":
            span_data["agent_info"] = {
                "agent_name": span.tags.get("agent.name"),
                "model": span.tags.get("agent.model"),
                "tools_count": span.tags.get("agent.tools_count", 0)
            }
        
        elif span.operation_name == "llm_generation":
            span_data["llm_info"] = {
                "model": span.tags.get("llm.model"),
                "input_tokens": span.tags.get("llm.input_tokens"),
                "output_tokens": span.tags.get("llm.output_tokens"),
                "total_tokens": span.tags.get("llm.total_tokens")
            }
        
        elif span.operation_name == "tool_call":
            span_data["tool_info"] = {
                "tool_name": span.tags.get("tool.name"),
                "tool_type": span.tags.get("tool.type"),
                "success": span.tags.get("tool.success", True)
            }
        
        # Check for errors
        if span.tags and span.tags.get("error"):
            error_info = {
                "trace_id": span.trace_id,
                "span_id": span.span_id,
                "error": span.tags.get("error"),
                "timestamp": datetime.now().isoformat()
            }
            self.error_logs.append(error_info)
            span_data["has_error"] = True
        
        return span_data
    
    def _calculate_duration(self, trace: Trace) -> float:
        """Calculate trace duration in milliseconds"""
        if trace.started_at and trace.ended_at:
            return (trace.ended_at - trace.started_at).total_seconds() * 1000
        return 0
    
    def _calculate_span_duration(self, span: Span) -> float:
        """Calculate span duration in milliseconds"""
        if span.started_at and span.ended_at:
            return (span.ended_at - span.started_at).total_seconds() * 1000
        return 0
    
    def _update_metrics(self, trace_data: Dict[str, Any]):
        """Update performance metrics"""
        
        workflow = trace_data["workflow_name"]
        duration = trace_data["duration_ms"]
        
        if workflow not in self.performance_metrics:
            self.performance_metrics[workflow] = []
        
        self.performance_metrics[workflow].append(duration)
        
        # Keep only last 100 measurements per workflow
        if len(self.performance_metrics[workflow]) > 100:
            self.performance_metrics[workflow] = self.performance_metrics[workflow][-100:]
    
    def _log_trace_summary(self, trace_data: Dict[str, Any]):
        """Log trace summary cho monitoring"""
        
        total_spans = len(trace_data["spans"])
        duration = trace_data["duration_ms"]
        workflow = trace_data["workflow_name"]
        
        print(f"ðŸ“ˆ Trace completed: {workflow} | {duration:.1f}ms | {total_spans} spans")
        
        # Log performance warnings
        if duration > 5000:  # More than 5 seconds
            print(f"âš ï¸  Slow trace detected: {workflow} took {duration:.1f}ms")
        
        # Log errors
        errors_in_trace = [span for span in trace_data["spans"] if span.get("has_error")]
        if errors_in_trace:
            print(f"âŒ Errors detected in trace: {len(errors_in_trace)} spans with errors")
    
    def get_analytics_report(self) -> Dict[str, Any]:
        """Generate comprehensive analytics report"""
        
        if not self.traces:
            return {"message": "No traces collected yet"}
        
        # Calculate summary statistics
        total_traces = len(self.traces)
        total_errors = len(self.error_logs)
        
        # Performance statistics
        all_durations = [trace["duration_ms"] for trace in self.traces]
        avg_duration = sum(all_durations) / len(all_durations) if all_durations else 0
        
        # Workflow statistics
        workflow_stats = {}
        for trace in self.traces:
            workflow = trace["workflow_name"]
            if workflow not in workflow_stats:
                workflow_stats[workflow] = {"count": 0, "total_duration": 0}
            
            workflow_stats[workflow]["count"] += 1
            workflow_stats[workflow]["total_duration"] += trace["duration_ms"]
        
        # Calculate averages cho má»—i workflow
        for workflow, stats in workflow_stats.items():
            stats["avg_duration"] = stats["total_duration"] / stats["count"]
        
        # Most common operations
        all_operations = []
        for trace in self.traces:
            for span in trace["spans"]:
                all_operations.append(span["operation_name"])
        
        operation_counts = {}
        for op in all_operations:
            operation_counts[op] = operation_counts.get(op, 0) + 1
        
        return {
            "summary": {
                "total_traces": total_traces,
                "total_errors": total_errors,
                "error_rate": total_errors / total_traces if total_traces > 0 else 0,
                "avg_duration_ms": avg_duration
            },
            "workflow_performance": workflow_stats,
            "operation_frequency": operation_counts,
            "recent_errors": self.error_logs[-10:],  # Last 10 errors
            "generated_at": datetime.now().isoformat()
        }

# Setup custom tracing
def setup_custom_tracing():
    """Setup custom trace processor"""
    
    detailed_processor = DetailedTraceProcessor()
    add_trace_processor(detailed_processor)
    
    return detailed_processor

# Demo custom tracing
async def demo_custom_tracing():
    """Demo custom tracing vá»›i detailed analytics"""
    
    print("ðŸ”§ **Custom Tracing Demo**\n")
    
    # Setup custom processor
    trace_processor = setup_custom_tracing()
    
    # Create agent vá»›i tools Ä‘á»ƒ generate interesting traces
    from agents import function_tool
    
    @function_tool
    def calculate_tax(amount: float, rate: float = 0.1) -> float:
        """Calculate tax on an amount"""
        return amount * rate
    
    @function_tool
    def format_currency(amount: float, currency: str = "VND") -> str:
        """Format amount as currency"""
        if currency == "VND":
            return f"{amount:,.0f} â‚«"
        elif currency == "USD":
            return f"${amount:,.2f}"
        else:
            return f"{amount:.2f} {currency}"
    
    tax_agent = Agent(
        name="Tax Calculator",
        instructions="""
        Báº¡n lÃ  má»™t chuyÃªn gia tÃ­nh thuáº¿.
        
        Capabilities:
        - TÃ­nh thuáº¿ cho cÃ¡c khoáº£n thu nháº­p
        - Format currency theo different formats
        - Provide tax advice vÃ  explanations
        
        Always:
        - Use tools Ä‘á»ƒ tÃ­nh toÃ¡n chÃ­nh xÃ¡c
        - Explain calculations step by step  
        - Format results clearly
        """,
        tools=[calculate_tax, format_currency]
    )
    
    # Test scenarios vá»›i different complexity levels
    test_scenarios = [
        {
            "name": "Simple Tax Calculation",
            "query": "TÃ­nh thuáº¿ cho thu nháº­p 50,000,000 VND vá»›i tax rate 10%"
        },
        {
            "name": "Multi-step Calculation",
            "query": "TÃ´i cÃ³ thu nháº­p $5000 USD. TÃ­nh thuáº¿ 15% vÃ  convert sang VND (tá»· giÃ¡ 24,000)"
        },
        {
            "name": "Complex Scenario",
            "query": "So sÃ¡nh tax burden giá»¯a thu nháº­p 100 triá»‡u VND vÃ  $4000 USD vá»›i tax rates khÃ¡c nhau"
        }
    ]
    
    for scenario in test_scenarios:
        print(f"ðŸ’¼ **Scenario:** {scenario['name']}")
        print(f"ðŸ“ **Query:** {scenario['query']}")
        
        # Run vá»›i custom workflow name Ä‘á»ƒ tracking
        with trace(workflow_name=scenario['name']):
            result = await Runner.run(tax_agent, scenario['query'])
            print(f"ðŸ’¡ **Result:** {result.final_output[:200]}...")
        
        print("-" * 60 + "\n")
    
    # Generate analytics report
    await asyncio.sleep(1)  # Wait for traces to be processed
    
    analytics = trace_processor.get_analytics_report()
    
    print("ðŸ“Š **Analytics Report:**")
    print(f"ðŸ“ˆ Total Traces: {analytics['summary']['total_traces']}")
    print(f"â±ï¸  Average Duration: {analytics['summary']['avg_duration_ms']:.1f}ms")
    print(f"âŒ Error Rate: {analytics['summary']['error_rate']:.1%}")
    
    print(f"\nðŸŽ¯ **Workflow Performance:**")
    for workflow, stats in analytics['workflow_performance'].items():
        print(f"   â€¢ {workflow}: {stats['count']} runs, avg {stats['avg_duration']:.1f}ms")
    
    print(f"\nðŸ”§ **Operation Frequency:**")
    for operation, count in analytics['operation_frequency'].items():
        print(f"   â€¢ {operation}: {count} times")

if __name__ == "__main__":
    import asyncio
    asyncio.run(demo_custom_tracing())
```

## Advanced Debug Techniques

### Debug Agent Decision Making

```python
from agents import Agent, Runner, trace, function_tool
from typing import Dict, Any, List
import json

class AgentDecisionDebugger:
    """Debug agent decision making process"""
    
    def __init__(self):
        self.decision_log: List[Dict[str, Any]] = []
        self.tool_usage_stats: Dict[str, int] = {}
        self.handoff_patterns: List[Dict[str, Any]] = []
    
    def log_decision(self, decision_type: str, context: Dict[str, Any]):
        """Log agent decision vá»›i context"""
        
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "decision_type": decision_type, 
            "context": context,
            "session_id": context.get("session_id", "unknown")
        }
        
        self.decision_log.append(log_entry)
        
        # Update statistics
        if decision_type == "tool_selection":
            tool_name = context.get("tool_name")
            if tool_name:
                self.tool_usage_stats[tool_name] = self.tool_usage_stats.get(tool_name, 0) + 1
        
        elif decision_type == "handoff":
            self.handoff_patterns.append({
                "from_agent": context.get("from_agent"),
                "to_agent": context.get("to_agent"),
                "reason": context.get("reason"),
                "timestamp": log_entry["timestamp"]
            })
    
    def analyze_decision_patterns(self) -> Dict[str, Any]:
        """Analyze decision making patterns"""
        
        if not self.decision_log:
            return {"message": "No decisions logged yet"}
        
        # Decision type frequency
        decision_types = {}
        for decision in self.decision_log:
            dtype = decision["decision_type"]
            decision_types[dtype] = decision_types.get(dtype, 0) + 1
        
        # Tool selection patterns
        tool_analysis = {}
        if self.tool_usage_stats:
            total_tool_calls = sum(self.tool_usage_stats.values())
            for tool, count in self.tool_usage_stats.items():
                tool_analysis[tool] = {
                    "usage_count": count,
                    "usage_percentage": (count / total_tool_calls) * 100
                }
        
        # Handoff analysis
        handoff_analysis = {}
        if self.handoff_patterns:
            for handoff in self.handoff_patterns:
                pattern = f"{handoff['from_agent']} -> {handoff['to_agent']}"
                handoff_analysis[pattern] = handoff_analysis.get(pattern, 0) + 1
        
        return {
            "total_decisions": len(self.decision_log),
            "decision_type_frequency": decision_types,
            "tool_usage_analysis": tool_analysis,
            "handoff_patterns": handoff_analysis,
            "most_used_tool": max(self.tool_usage_stats.items(), key=lambda x: x[1])[0] if self.tool_usage_stats else None,
            "analysis_timestamp": datetime.now().isoformat()
        }

# Debug-enabled agents
class DebugAgent:
    """Agent wrapper vá»›i debugging capabilities"""
    
    def __init__(self, base_agent: Agent, debugger: AgentDecisionDebugger):
        self.base_agent = base_agent
        self.debugger = debugger
        self.session_id = f"debug_session_{int(time.time())}"
    
    async def run_with_debug(self, user_input: str) -> Dict[str, Any]:
        """Run agent vá»›i comprehensive debugging"""
        
        print(f"ðŸ› **Debug Session:** {self.session_id}")
        print(f"ðŸ“ **Input:** {user_input}")
        
        # Log initial decision context
        self.debugger.log_decision("input_processing", {
            "session_id": self.session_id,
            "input_length": len(user_input),
            "agent_name": self.base_agent.name,
            "tools_available": len(self.base_agent.tools or [])
        })
        
        # Run vá»›i tracing
        with trace(workflow_name=f"Debug: {self.base_agent.name}"):
            start_time = time.time()
            
            try:
                result = await Runner.run(self.base_agent, user_input)
                
                execution_time = time.time() - start_time
                
                # Log successful completion
                self.debugger.log_decision("execution_complete", {
                    "session_id": self.session_id,
                    "execution_time": execution_time,
                    "output_length": len(result.final_output),
                    "tools_called": len([item for item in result.new_items if item.type == "tool_call_item"]),
                    "success": True
                })
                
                # Analyze tools used
                for item in result.new_items:
                    if item.type == "tool_call_item":
                        self.debugger.log_decision("tool_selection", {
                            "session_id": self.session_id,
                            "tool_name": item.tool_name if hasattr(item, 'tool_name') else "unknown",
                            "execution_order": len([i for i in result.new_items[:result.new_items.index(item)] if i.type == "tool_call_item"]) + 1
                        })
                
                print(f"âœ… **Success:** {execution_time:.2f}s")
                
                return {
                    "success": True,
                    "result": result,
                    "debug_info": {
                        "session_id": self.session_id,
                        "execution_time": execution_time,
                        "tools_used": len([item for item in result.new_items if item.type == "tool_call_item"])
                    }
                }
                
            except Exception as e:
                execution_time = time.time() - start_time
                
                # Log error
                self.debugger.log_decision("execution_error", {
                    "session_id": self.session_id,
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "execution_time": execution_time
                })
                
                print(f"âŒ **Error:** {str(e)}")
                
                return {
                    "success": False,
                    "error": str(e),
                    "debug_info": {
                        "session_id": self.session_id,
                        "execution_time": execution_time,
                        "error_type": type(e).__name__
                    }
                }

# Demo debug techniques
async def demo_debug_techniques():
    """Demo advanced debugging techniques"""
    
    print("ðŸ” **Advanced Agent Debugging Demo**\n")
    
    # Setup debugger
    debugger = AgentDecisionDebugger()
    
    # Create agents vá»›i different behaviors
    @function_tool
    def search_product_database(query: str, category: str = "all") -> str:
        """Search for products in database"""
        # Simulate database search
        products = {
            "laptop": ["Dell XPS 13", "MacBook Pro", "ThinkPad X1"],
            "phone": ["iPhone 15", "Samsung Galaxy S24", "Google Pixel 8"],
            "headphones": ["AirPods Pro", "Sony WH-1000XM5", "Bose QuietComfort"]
        }
        
        if category in products:
            return f"Found products in {category}: {', '.join(products[category])}"
        else:
            return f"Search results for '{query}': Found 12 products across all categories"
    
    @function_tool
    def check_inventory(product_name: str) -> str:
        """Check product inventory status"""
        # Simulate inventory check
        import random
        stock = random.randint(0, 50)
        status = "In Stock" if stock > 5 else "Low Stock" if stock > 0 else "Out of Stock"
        return f"{product_name}: {status} ({stock} units available)"
    
    @function_tool
    def get_price_info(product_name: str) -> str:
        """Get product pricing information"""
        # Simulate price lookup
        import random
        base_price = random.randint(500, 5000) * 1000  # VND
        discount = random.choice([0, 5, 10, 15, 20])
        
        if discount > 0:
            discounted_price = base_price * (1 - discount/100)
            return f"{product_name}: {base_price:,} VND (Sale: {discounted_price:,} VND - {discount}% off)"
        else:
            return f"{product_name}: {base_price:,} VND"
    
    # Create product assistant
    product_assistant = Agent(
        name="Product Research Assistant",
        instructions="""
        Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n sáº£n pháº©m thÃ´ng minh.
        
        Process:
        1. Hiá»ƒu requirements cá»§a customer
        2. Search relevant products
        3. Check inventory status
        4. Get pricing information
        5. Provide comprehensive recommendation
        
        Decision making:
        - Always search products first
        - Check inventory cho popular items
        - Get pricing cho final recommendations
        - Consider customer budget vÃ  needs
        
        Be helpful, thorough, and data-driven.
        """,
        tools=[search_product_database, check_inventory, get_price_info]
    )
    
    # Wrap vá»›i debug capabilities
    debug_agent = DebugAgent(product_assistant, debugger)
    
    # Test scenarios vá»›i increasing complexity
    debug_scenarios = [
        "TÃ´i cáº§n má»™t laptop cho há»c láº­p trÃ¬nh, budget khoáº£ng 30 triá»‡u",
        "TÃ¬m tai nghe bluetooth tá»‘t nháº¥t hiá»‡n táº¡i, khÃ´ng quan tÃ¢m giÃ¡",
        "So sÃ¡nh iPhone 15 vÃ  Samsung Galaxy S24, cáº£ vá» giÃ¡ vÃ  tÃ­nh nÄƒng",
        "Recommend smartphone táº§m trung, dÆ°á»›i 15 triá»‡u, pin tá»‘t"
    ]
    
    for i, scenario in enumerate(debug_scenarios, 1):
        print(f"ðŸŽ¯ **Debug Scenario {i}:**")
        
        debug_result = await debug_agent.run_with_debug(scenario)
        
        if debug_result["success"]:
            result = debug_result["result"]
            debug_info = debug_result["debug_info"]
            
            print(f"ðŸ“Š **Debug Info:**")
            print(f"   â€¢ Session: {debug_info['session_id']}")
            print(f"   â€¢ Execution Time: {debug_info['execution_time']:.2f}s")
            print(f"   â€¢ Tools Used: {debug_info['tools_used']}")
            
            print(f"ðŸ’¡ **Response:** {result.final_output[:300]}...")
        
        else:
            print(f"âŒ **Failed:** {debug_result['error']}")
        
        print("=" * 70 + "\n")
    
    # Analyze decision patterns
    analysis = debugger.analyze_decision_patterns()
    
    print("ðŸ“ˆ **Decision Analysis:**")
    print(f"ðŸ“Š Total Decisions: {analysis['total_decisions']}")
    
    if analysis.get('tool_usage_analysis'):
        print(f"\nðŸ”§ **Tool Usage Patterns:**")
        for tool, stats in analysis['tool_usage_analysis'].items():
            print(f"   â€¢ {tool}: {stats['usage_count']} times ({stats['usage_percentage']:.1f}%)")
        
        if analysis.get('most_used_tool'):
            print(f"\nâ­ **Most Used Tool:** {analysis['most_used_tool']}")
    
    if analysis.get('decision_type_frequency'):
        print(f"\nðŸŽ¯ **Decision Type Frequency:**")
        for dtype, count in analysis['decision_type_frequency'].items():
            print(f"   â€¢ {dtype}: {count} times")

if __name__ == "__main__":
    import time
    from datetime import datetime
    import asyncio
    asyncio.run(demo_debug_techniques())
```

## Production Monitoring vÃ  Alerting

### Comprehensive Monitoring System

```python
import asyncio
import time
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import statistics

class AlertLevel(Enum):
    INFO = "info"
    WARNING = "warning"  
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class Alert:
    level: AlertLevel
    message: str
    metric_name: str
    current_value: float
    threshold: float
    timestamp: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "level": self.level.value,
            "message": self.message,
            "metric_name": self.metric_name,
            "current_value": self.current_value,
            "threshold": self.threshold,
            "timestamp": self.timestamp.isoformat()
        }

class ProductionMonitor:
    """Comprehensive production monitoring system"""
    
    def __init__(self):
        # Metrics storage
        self.metrics: Dict[str, List[float]] = {}
        self.metric_timestamps: Dict[str, List[datetime]] = {}
        
        # Alert configuration
        self.alert_thresholds = {
            "response_time_ms": {"warning": 2000, "error": 5000, "critical": 10000},
            "error_rate": {"warning": 0.05, "error": 0.10, "critical": 0.20},
            "token_throughput": {"warning": 5, "error": 2, "critical": 1},  # tokens/sec
            "concurrent_requests": {"warning": 50, "error": 100, "critical": 150}
        }
        
        # Alert history
        self.alerts: List[Alert] = []
        self.alert_cooldown: Dict[str, datetime] = {}
        
        # System health
        self.last_health_check = datetime.now()
        self.health_status = "healthy"
        
        print("ðŸš¨ Production Monitor initialized")
    
    def record_metric(self, metric_name: str, value: float):
        """Record a metric value"""
        
        if metric_name not in self.metrics:
            self.metrics[metric_name] = []
            self.metric_timestamps[metric_name] = []
        
        self.metrics[metric_name].append(value)
        self.metric_timestamps[metric_name].append(datetime.now())
        
        # Keep only last 1000 data points
        if len(self.metrics[metric_name]) > 1000:
            self.metrics[metric_name] = self.metrics[metric_name][-1000:]
            self.metric_timestamps[metric_name] = self.metric_timestamps[metric_name][-1000:]
        
        # Check for alerts
        self._check_alerts(metric_name, value)
    
    def _check_alerts(self, metric_name: str, current_value: float):
        """Check if metric value triggers any alerts"""
        
        if metric_name not in self.alert_thresholds:
            return
        
        thresholds = self.alert_thresholds[metric_name]
        
        # Check alert levels (tá»« critical xuá»‘ng info)
        alert_level = None
        threshold_value = None
        
        if "critical" in thresholds and current_value >= thresholds["critical"]:
            alert_level = AlertLevel.CRITICAL
            threshold_value = thresholds["critical"]
        elif "error" in thresholds and current_value >= thresholds["error"]:
            alert_level = AlertLevel.ERROR  
            threshold_value = thresholds["error"]
        elif "warning" in thresholds and current_value >= thresholds["warning"]:
            alert_level = AlertLevel.WARNING
            threshold_value = thresholds["warning"]
        
        if alert_level:
            # Check cooldown Ä‘á»ƒ avoid spam alerts
            cooldown_key = f"{metric_name}_{alert_level.value}"
            now = datetime.now()
            
            if cooldown_key in self.alert_cooldown:
                if now - self.alert_cooldown[cooldown_key] < timedelta(minutes=5):
                    return  # Still in cooldown
            
            # Create alert
            alert = Alert(
                level=alert_level,
                message=f"{metric_name} is {current_value:.2f}, exceeds {alert_level.value} threshold of {threshold_value}",
                metric_name=metric_name,
                current_value=current_value,
                threshold=threshold_value
            )
            
            self.alerts.append(alert)
            self.alert_cooldown[cooldown_key] = now
            
            # Trigger alert notification
            self._trigger_alert(alert)
    
    def _trigger_alert(self, alert: Alert):
        """Trigger alert notification"""
        
        # Print alert (trong production sáº½ gá»­i Ä‘áº¿n Slack, email, etc.)
        icon = {
            AlertLevel.INFO: "â„¹ï¸",
            AlertLevel.WARNING: "âš ï¸", 
            AlertLevel.ERROR: "âŒ",
            AlertLevel.CRITICAL: "ðŸš¨"
        }
        
        print(f"\n{icon[alert.level]} **ALERT [{alert.level.value.upper()}]**")
        print(f"   ðŸ“Š {alert.message}")
        print(f"   ðŸ• {alert.timestamp.strftime('%H:%M:%S')}")
        
        # Update system health status
        if alert.level in [AlertLevel.ERROR, AlertLevel.CRITICAL]:
            self.health_status = "degraded" if alert.level == AlertLevel.ERROR else "critical"
    
    def get_metric_summary(self, metric_name: str, minutes: int = 60) -> Dict[str, Any]:
        """Get metric summary for specified time window"""
        
        if metric_name not in self.metrics:
            return {"error": f"Metric {metric_name} not found"}
        
        # Filter data within time window
        now = datetime.now()
        cutoff_time = now - timedelta(minutes=minutes)
        
        values = []
        timestamps = []
        
        for i, timestamp in enumerate(self.metric_timestamps[metric_name]):
            if timestamp >= cutoff_time:
                values.append(self.metrics[metric_name][i])
                timestamps.append(timestamp)
        
        if not values:
            return {"message": f"No data for {metric_name} in last {minutes} minutes"}
        
        return {
            "metric_name": metric_name,
            "time_window_minutes": minutes,
            "data_points": len(values),
            "latest_value": values[-1],
            "min_value": min(values),
            "max_value": max(values),
            "avg_value": statistics.mean(values),
            "median_value": statistics.median(values),
            "std_dev": statistics.stdev(values) if len(values) > 1 else 0,
            "trend": self._calculate_trend(values),
            "last_updated": timestamps[-1].isoformat() if timestamps else None
        }
    
    def _calculate_trend(self, values: List[float]) -> str:
        """Calculate trend direction"""
        if len(values) < 2:
            return "insufficient_data"
        
        # Simple trend calculation
        first_half = values[:len(values)//2]
        second_half = values[len(values)//2:]
        
        first_avg = statistics.mean(first_half)
        second_avg = statistics.mean(second_half)
        
        diff_percent = ((second_avg - first_avg) / first_avg) * 100 if first_avg != 0 else 0
        
        if abs(diff_percent) < 5:
            return "stable"
        elif diff_percent > 0:
            return "increasing"
        else:
            return "decreasing"
    
    def get_system_health(self) -> Dict[str, Any]:
        """Get overall system health status"""
        
        recent_alerts = [a for a in self.alerts if datetime.now() - a.timestamp < timedelta(hours=1)]
        critical_alerts = [a for a in recent_alerts if a.level == AlertLevel.CRITICAL]
        error_alerts = [a for a in recent_alerts if a.level == AlertLevel.ERROR]
        
        # Calculate health score
        health_score = 100
        health_score -= len(critical_alerts) * 30
        health_score -= len(error_alerts) * 15
        health_score -= len([a for a in recent_alerts if a.level == AlertLevel.WARNING]) * 5
        health_score = max(0, health_score)
        
        return {
            "status": self.health_status,
            "health_score": health_score,
            "last_check": self.last_health_check.isoformat(),
            "alerts_last_hour": {
                "critical": len(critical_alerts),
                "error": len(error_alerts),
                "warning": len([a for a in recent_alerts if a.level == AlertLevel.WARNING]),
                "total": len(recent_alerts)
            },
            "metrics_tracked": len(self.metrics),
            "uptime_status": "operational"  # Trong production sáº½ track actual uptime
        }
    
    def generate_dashboard_data(self) -> Dict[str, Any]:
        """Generate data cho monitoring dashboard"""
        
        dashboard = {
            "timestamp": datetime.now().isoformat(),
            "system_health": self.get_system_health(),
            "key_metrics": {},
            "recent_alerts": [alert.to_dict() for alert in self.alerts[-10:]],
            "metric_summaries": {}
        }
        
        # Get summaries cho key metrics
        key_metrics = ["response_time_ms", "error_rate", "token_throughput"]
        
        for metric in key_metrics:
            if metric in self.metrics:
                summary = self.get_metric_summary(metric, minutes=60)
                dashboard["metric_summaries"][metric] = summary
                
                # Add current value to key metrics
                if "latest_value" in summary:
                    dashboard["key_metrics"][metric] = summary["latest_value"]
        
        return dashboard

# Production monitoring integration
class MonitoredAgent:
    """Agent wrapper vá»›i production monitoring"""
    
    def __init__(self, agent: Agent, monitor: ProductionMonitor):
        self.agent = agent
        self.monitor = monitor
        self.request_count = 0
        self.concurrent_requests = 0
    
    async def run_with_monitoring(self, user_input: str) -> Dict[str, Any]:
        """Run agent vá»›i comprehensive monitoring"""
        
        self.request_count += 1
        self.concurrent_requests += 1
        
        # Record concurrent requests
        self.monitor.record_metric("concurrent_requests", self.concurrent_requests)
        
        start_time = time.time()
        tokens_generated = 0
        errors_occurred = 0
        
        try:
            # Run agent vá»›i monitoring
            result = await Runner.run(self.agent, user_input)
            
            # Calculate metrics
            response_time = (time.time() - start_time) * 1000  # ms
            tokens_generated = len(result.final_output.split())  # Approximate token count
            token_throughput = tokens_generated / (response_time / 1000) if response_time > 0 else 0
            
            # Record metrics
            self.monitor.record_metric("response_time_ms", response_time)
            self.monitor.record_metric("token_throughput", token_throughput)
            self.monitor.record_metric("error_rate", 0)  # Success
            
            return {
                "success": True,
                "result": result,
                "metrics": {
                    "response_time_ms": response_time,
                    "tokens_generated": tokens_generated,
                    "token_throughput": token_throughput
                }
            }
            
        except Exception as e:
            errors_occurred = 1
            response_time = (time.time() - start_time) * 1000
            
            # Record error metrics
            self.monitor.record_metric("response_time_ms", response_time)
            self.monitor.record_metric("error_rate", 1)  # Error occurred
            
            return {
                "success": False,
                "error": str(e),
                "metrics": {
                    "response_time_ms": response_time,
                    "errors": errors_occurred
                }
            }
            
        finally:
            self.concurrent_requests -= 1

# Demo production monitoring
async def demo_production_monitoring():
    """Demo comprehensive production monitoring"""
    
    print("ðŸ“Š **Production Monitoring Demo**\n")
    
    # Setup monitoring
    monitor = ProductionMonitor()
    
    # Create monitored agent
    production_agent = Agent(
        name="Production Assistant",
        instructions="""
        Báº¡n lÃ  má»™t production assistant Ä‘Æ°á»£c monitor 24/7.
        
        Provide helpful, accurate responses while maintaining high performance.
        Be concise but informative.
        """,
        tools=[]
    )
    
    monitored_agent = MonitoredAgent(production_agent, monitor)
    
    # Simulate production traffic
    print("ðŸš€ Simulating production traffic...\n")
    
    test_queries = [
        "Explain machine learning briefly",
        "What is cloud computing?", 
        "How does blockchain work?",
        "Benefits of microservices architecture",
        "Database optimization techniques"
    ] * 3  # Repeat to generate more data
    
    # Process requests vá»›i varying delays Ä‘á»ƒ simulate real traffic
    for i, query in enumerate(test_queries):
        print(f"ðŸ”„ Request {i+1}: {query[:50]}...")
        
        # Simulate variable response times
        if i % 7 == 0:  # Occasionally slow request
            await asyncio.sleep(0.5)  # Add artificial delay
        
        result = await monitored_agent.run_with_monitoring(query)
        
        if result["success"]:
            metrics = result["metrics"]
            print(f"   âœ… {metrics['response_time_ms']:.0f}ms | {metrics['tokens_generated']} tokens | {metrics['token_throughput']:.1f} t/s")
        else:
            print(f"   âŒ Error: {result['error']}")
        
        # Small delay between requests
        await asyncio.sleep(0.1)
    
    print("\n" + "="*60 + "\n")
    
    # Generate dashboard
    dashboard = monitor.generate_dashboard_data()
    
    print("ðŸ“ˆ **Production Dashboard:**")
    
    # System health
    health = dashboard["system_health"]
    print(f"ðŸ¥ **System Health:** {health['status'].upper()}")
    print(f"   â€¢ Health Score: {health['health_score']}/100")
    print(f"   â€¢ Metrics Tracked: {health['metrics_tracked']}")
    
    # Recent alerts
    recent_alerts = dashboard["recent_alerts"]
    if recent_alerts:
        print(f"\nðŸš¨ **Recent Alerts:** ({len(recent_alerts)} alerts)")
        for alert in recent_alerts[-3:]:  # Show last 3
            print(f"   â€¢ {alert['level'].upper()}: {alert['message']}")
    else:
        print(f"\nâœ… **No Recent Alerts**")
    
    # Key metrics
    key_metrics = dashboard["key_metrics"]
    if key_metrics:
        print(f"\nðŸ“Š **Key Metrics:**")
        for metric, value in key_metrics.items():
            if metric == "response_time_ms":
                print(f"   â€¢ Response Time: {value:.0f}ms")
            elif metric == "token_throughput":
                print(f"   â€¢ Token Throughput: {value:.1f} tokens/sec")
            elif metric == "error_rate":
                print(f"   â€¢ Error Rate: {value:.1%}")
    
    # Metric summaries
    summaries = dashboard["metric_summaries"]
    if summaries:
        print(f"\nðŸ“ˆ **Metric Trends (Last Hour):**")
        for metric, summary in summaries.items():
            trend_icon = {"increasing": "ðŸ“ˆ", "decreasing": "ðŸ“‰", "stable": "âž¡ï¸"}.get(summary["trend"], "â“")
            print(f"   â€¢ {metric}: {summary['avg_value']:.1f} avg, {summary['trend']} {trend_icon}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(demo_production_monitoring())
```

## Kháº¯c Phá»¥c Sá»± Cá»‘ ThÆ°á»ng Gáº·p

### Common Issues vÃ  Solutions

```python
from typing import List, Dict, Any, Optional
import re
import json

class TroubleshootingGuide:
    """HÆ°á»›ng dáº«n kháº¯c phá»¥c sá»± cá»‘ thÆ°á»ng gáº·p"""
    
    def __init__(self):
        self.issue_patterns = {
            "slow_response": {
                "symptoms": ["response_time > 5000ms", "timeout errors", "user complaints about speed"],
                "causes": ["complex prompts", "too many tools", "inefficient tool implementations", "network latency"],
                "solutions": [
                    "Optimize agent instructions",
                    "Reduce number of tools",
                    "Implement tool caching",
                    "Use streaming responses",
                    "Add performance monitoring"
                ]
            },
            
            "high_error_rate": {
                "symptoms": ["error_rate > 10%", "frequent exceptions", "incomplete responses"],
                "causes": ["API rate limits", "invalid tool schemas", "network issues", "malformed inputs"],
                "solutions": [
                    "Implement retry logic with exponential backoff",
                    "Validate tool schemas",
                    "Add input sanitization",
                    "Improve error handling",
                    "Monitor API quotas"
                ]
            },
            
            "unexpected_behavior": {
                "symptoms": ["wrong tool selection", "poor handoff decisions", "irrelevant responses"],
                "causes": ["unclear instructions", "insufficient context", "tool description ambiguity"],
                "solutions": [
                    "Refine agent instructions",
                    "Improve tool descriptions",
                    "Add more context to prompts",
                    "Use examples in instructions",
                    "Implement decision logging"
                ]
            },
            
            "resource_exhaustion": {
                "symptoms": ["memory errors", "CPU spikes", "concurrent request failures"],
                "causes": ["memory leaks", "inefficient algorithms", "too many concurrent requests"],
                "solutions": [
                    "Implement connection pooling",
                    "Add request queuing",
                    "Optimize memory usage",
                    "Scale horizontally",
                    "Add resource monitoring"
                ]
            }
        }
    
    def diagnose_issue(self, symptoms: List[str], metrics: Dict[str, float] = None) -> Dict[str, Any]:
        """Cháº©n Ä‘oÃ¡n váº¥n Ä‘á» dá»±a trÃªn symptoms vÃ  metrics"""
        
        potential_issues = []
        
        # Check tá»«ng issue pattern
        for issue_type, pattern in self.issue_patterns.items():
            match_score = 0
            
            # Check symptoms
            for symptom in symptoms:
                if any(pattern_symptom in symptom.lower() for pattern_symptom in pattern["symptoms"]):
                    match_score += 1
            
            # Check metrics náº¿u cÃ³
            if metrics:
                if issue_type == "slow_response" and metrics.get("response_time_ms", 0) > 5000:
                    match_score += 2
                elif issue_type == "high_error_rate" and metrics.get("error_rate", 0) > 0.1:
                    match_score += 2
            
            if match_score > 0:
                potential_issues.append({
                    "issue_type": issue_type,
                    "match_score": match_score,
                    "confidence": min(match_score / len(pattern["symptoms"]), 1.0),
                    "pattern": pattern
                })
        
        # Sort by confidence
        potential_issues.sort(key=lambda x: x["confidence"], reverse=True)
        
        return {
            "diagnosis": potential_issues,
            "top_issue": potential_issues[0] if potential_issues else None,
            "recommendations": self._generate_recommendations(potential_issues)
        }
    
    def _generate_recommendations(self, issues: List[Dict[str, Any]]) -> List[str]:
        """Generate actionable recommendations"""
        
        if not issues:
            return ["No specific issues detected. Continue monitoring system health."]
        
        recommendations = []
        
        # Get top 2 issues
        for issue in issues[:2]:
            issue_type = issue["issue_type"]
            solutions = issue["pattern"]["solutions"]
            
            recommendations.append(f"For {issue_type.replace('_', ' ')}:")
            for solution in solutions[:3]:  # Top 3 solutions
                recommendations.append(f"  â€¢ {solution}")
        
        return recommendations
    
    def get_troubleshooting_checklist(self, issue_type: str) -> List[str]:
        """Get step-by-step troubleshooting checklist"""
        
        checklists = {
            "slow_response": [
                "Check average response time metrics",
                "Review agent instructions for complexity",
                "Count number of tools available to agent",
                "Test individual tool performance",
                "Monitor network latency",
                "Enable streaming if not already active",
                "Review tracing data for bottlenecks"
            ],
            
            "high_error_rate": [
                "Check error logs for common patterns",
                "Verify API keys and authentication",
                "Test tool schemas with sample data",
                "Review input validation logic",
                "Check rate limit status",
                "Verify network connectivity",
                "Test error recovery mechanisms"
            ],
            
            "unexpected_behavior": [
                "Review agent instructions for clarity",
                "Test with sample inputs",
                "Check tool descriptions accuracy",
                "Verify context is being passed correctly",
                "Review handoff logic",
                "Test edge cases",
                "Enable decision logging"
            ],
            
            "resource_exhaustion": [
                "Monitor memory usage patterns",
                "Check CPU utilization",
                "Review concurrent request limits",
                "Analyze connection pool usage",
                "Check for memory leaks",
                "Review garbage collection metrics",
                "Consider horizontal scaling"
            ]
        }
        
        return checklists.get(issue_type, ["No specific checklist available for this issue type."])

# Demo troubleshooting system
async def demo_troubleshooting():
    """Demo troubleshooting system"""
    
    print("ðŸ”§ **Agent Troubleshooting Demo**\n")
    
    troubleshooter = TroubleshootingGuide()
    
    # Test scenarios
    scenarios = [
        {
            "name": "Slow Performance Issue",
            "symptoms": ["responses taking over 10 seconds", "users complaining about speed", "timeout errors"],
            "metrics": {"response_time_ms": 8500, "error_rate": 0.02}
        },
        {
            "name": "High Error Rate",
            "symptoms": ["many failed requests", "API errors", "incomplete responses"],
            "metrics": {"response_time_ms": 2000, "error_rate": 0.15}
        },
        {
            "name": "Unexpected Agent Behavior", 
            "symptoms": ["wrong tool selections", "irrelevant responses", "poor handoff decisions"],
            "metrics": {"response_time_ms": 1500, "error_rate": 0.05}
        },
        {
            "name": "Resource Issues",
            "symptoms": ["memory errors", "CPU spikes", "concurrent request failures"],
            "metrics": {"response_time_ms": 3000, "error_rate": 0.08, "memory_usage": 0.95}
        }
    ]
    
    for scenario in scenarios:
        print(f"ðŸš¨ **Scenario:** {scenario['name']}")
        print(f"ðŸ“‹ **Symptoms:** {', '.join(scenario['symptoms'])}")
        print(f"ðŸ“Š **Metrics:** {scenario['metrics']}")
        
        # Diagnose issue
        diagnosis = troubleshooter.diagnose_issue(scenario['symptoms'], scenario['metrics'])
        
        if diagnosis['top_issue']:
            top_issue = diagnosis['top_issue']
            print(f"\nðŸŽ¯ **Primary Issue:** {top_issue['issue_type'].replace('_', ' ').title()}")
            print(f"ðŸ“ˆ **Confidence:** {top_issue['confidence']:.0%}")
            
            # Show recommendations
            print(f"\nðŸ’¡ **Recommendations:**")
            for rec in diagnosis['recommendations']:
                print(f"   {rec}")
            
            # Show troubleshooting checklist
            checklist = troubleshooter.get_troubleshooting_checklist(top_issue['issue_type'])
            print(f"\nâœ… **Troubleshooting Checklist:**")
            for i, step in enumerate(checklist, 1):
                print(f"   {i}. {step}")
        
        else:
            print(f"\nâ“ **No specific issues detected**")
        
        print("\n" + "="*70 + "\n")
    
    # Show general best practices
    print("ðŸ“‹ **General Best Practices:**")
    best_practices = [
        "Monitor key metrics continuously (response time, error rate, throughput)",
        "Implement comprehensive logging and tracing",
        "Set up automated alerts for critical thresholds",
        "Regular performance testing under load",
        "Keep agent instructions clear and specific",
        "Validate all tool schemas and implementations",
        "Implement graceful error handling and recovery",
        "Document known issues and their solutions"
    ]
    
    for i, practice in enumerate(best_practices, 1):
        print(f"   {i}. {practice}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(demo_troubleshooting())
```

## Nhá»¯ng Äiá»u Quan Trá»ng Cáº§n Nhá»›

### Kiáº¿n Thá»©c Cá»‘t LÃµi

âœ… **Tracing tá»± Ä‘á»™ng Ä‘Æ°á»£c báº­t** - Táº¥t cáº£ agent runs Ä‘á»u Ä‘Æ°á»£c theo dÃµi máº·c Ä‘á»‹nh  
âœ… **Hai loáº¡i events chÃ­nh** - Raw response events vÃ  run item events  
âœ… **Custom processors máº¡nh máº½** - CÃ³ thá»ƒ táº¡o monitoring systems phá»©c táº¡p  
âœ… **Production monitoring cáº§n thiáº¿t** - KhÃ´ng thá»ƒ thiáº¿u cho há»‡ thá»‘ng thá»±c táº¿  
âœ… **Debug techniques Ä‘a dáº¡ng** - Tá»« decision logging Ä‘áº¿n performance analysis  

### Kinh Nghiá»‡m Thá»±c Tiá»…n

ðŸŽ¯ **Tráº£i Nghiá»‡m NgÆ°á»i DÃ¹ng:**
- Hiá»ƒn thá»‹ feedback ngay láº­p tá»©c khi cÃ³ thá»ƒ
- DÃ¹ng cÃ¡c chá»‰ bÃ¡o tiáº¿n Ä‘á»™ rÃµ rÃ ng  
- Cho phÃ©p ngÆ°á»i dÃ¹ng dá»«ng cÃ¡c pháº£n há»“i dÃ i
- Cung cáº¥p thÃ´ng tin vá» nhá»¯ng gÃ¬ Ä‘ang xáº£y ra
- Xá»­ lÃ½ lá»—i má»™t cÃ¡ch nháº¹ nhÃ ng vá»›i cÃ¡c tÃ¹y chá»n khÃ´i phá»¥c

âš¡ **Hiá»‡u Suáº¥t:**
- Theo dÃµi thá»i gian Ä‘áº¿n token Ä‘áº§u tiÃªn nhÆ° má»™t chá»‰ sá»‘ quan trá»ng
- Tá»‘i Æ°u hÃ³a Ä‘á»ƒ cÃ³ tá»‘c Ä‘á»™ token á»•n Ä‘á»‹nh
- Triá»ƒn khai cÃ¡c chiáº¿n lÆ°á»£c Ä‘á»‡m thÃ´ng minh
- LÆ°u cache cÃ¡c pháº£n há»“i Ä‘Æ°á»£c yÃªu cáº§u thÆ°á»ng xuyÃªn
- Sá»­ dá»¥ng connection pooling Ä‘á»ƒ duy trÃ¬ hiá»‡u suáº¥t

ðŸ”§ **Triá»ƒn Khai:**
- Xá»­ lÃ½ cáº£ raw vÃ  structured streaming events
- Triá»ƒn khai khÃ´i phá»¥c lá»—i toÃ n diá»‡n
- ThÃªm giÃ¡m sÃ¡t hiá»‡u suáº¥t tá»« Ä‘áº§u
- Thiáº¿t káº¿ cho cáº£ giao diá»‡n console vÃ  web
- Kiá»ƒm tra vá»›i cÃ¡c Ä‘iá»u kiá»‡n máº¡ng khÃ¡c nhau

### HÆ°á»›ng Dáº«n Triá»ƒn Khai Sáº£n Xuáº¥t

ðŸ“Š **Thiáº¿t Láº­p GiÃ¡m SÃ¡t:**
- Theo dÃµi thá»i gian Ä‘áº¿n token Ä‘áº§u tiÃªn cho táº¥t cáº£ yÃªu cáº§u
- GiÃ¡m sÃ¡t tá»‘c Ä‘á»™ token vÃ  tá»· lá»‡ lá»—i
- Thiáº¿t láº­p cáº£nh bÃ¡o cho sá»± suy giáº£m hiá»‡u suáº¥t
- Ghi láº¡i cÃ¡c giÃ¡n Ä‘oáº¡n streaming vÃ  thÃ nh cÃ´ng khÃ´i phá»¥c
- Äo lÆ°á»ng sá»± tÆ°Æ¡ng tÃ¡c cá»§a ngÆ°á»i dÃ¹ng vá»›i streaming vs non-streaming

ðŸ›¡ï¸ **Xá»­ LÃ½ Lá»—i:**
- Triá»ƒn khai exponential backoff cho viá»‡c thá»­ láº¡i
- Báº£o tá»“n cÃ¡c pháº£n há»“i má»™t pháº§n trong khi cÃ³ lá»—i
- Cung cáº¥p thÃ´ng bÃ¡o lá»—i cÃ³ Ã½ nghÄ©a cho ngÆ°á»i dÃ¹ng
- Cho phÃ©p cÃ¡c tÃ¹y chá»n thá»­ láº¡i thá»§ cÃ´ng
- Theo dÃµi cÃ¡c máº«u lá»—i Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c váº¥n Ä‘á» há»‡ thá»‘ng

ðŸ”„ **Chiáº¿n LÆ°á»£c Tá»‘i Æ¯u:**
- LÃ m áº¥m trÆ°á»›c cÃ¡c káº¿t ná»‘i Ä‘á»ƒ giáº£m Ä‘á»™ trá»…
- Triá»ƒn khai caching thÃ´ng minh cho cÃ¡c truy váº¥n láº·p láº¡i
- Sá»­ dá»¥ng connection multiplexing khi cÃ³ thá»ƒ
- Tá»‘i Æ°u hÃ³a cÃ i Ä‘áº·t model cho hiá»‡u suáº¥t streaming
- Xem xÃ©t triá»ƒn khai edge Ä‘á»ƒ giáº£m Ä‘á»™ trá»… máº¡ng

### Nhá»¯ng Lá»—i ThÆ°á»ng Gáº·p VÃ  CÃ¡ch Kháº¯c Phá»¥c

âŒ **Bá» Qua Äá»™ Trá»… Token Äáº§u TiÃªn** â†’ GiÃ¡m sÃ¡t vÃ  tá»‘i Æ°u thá»i gian Ä‘áº¿n token Ä‘áº§u tiÃªn  
âŒ **KhÃ´ng CÃ³ KhÃ´i Phá»¥c Lá»—i** â†’ Triá»ƒn khai cÆ¡ cháº¿ thá»­ láº¡i máº¡nh máº½  
âŒ **Chá»‰ BÃ¡o Tiáº¿n Äá»™ KÃ©m** â†’ Hiá»ƒn thá»‹ tiáº¿n Ä‘á»™ vÃ  cáº­p nháº­t tráº¡ng thÃ¡i rÃµ rÃ ng  
âŒ **LÃ m ChoÃ¡ng Ngá»£p NgÆ°á»i DÃ¹ng** â†’ CÃ¢n báº±ng thÃ´ng tin vá»›i kháº£ nÄƒng sá»­ dá»¥ng  
âŒ **KhÃ´ng GiÃ¡m SÃ¡t Hiá»‡u Suáº¥t** â†’ Theo dÃµi cÃ¡c chá»‰ sá»‘ tá»« ngÃ y Ä‘áº§u tiÃªn  

### CÃ¡c Máº«u Streaming NÃ¢ng Cao

ðŸŽ¨ **Cáº£i Tiáº¿n Giao Diá»‡n NgÆ°á»i DÃ¹ng:**
- Tiáº¿t lá»™ tiáº¿n dáº§n - hiá»ƒn thá»‹ tÃ³m táº¯t trÆ°á»›c, chi tiáº¿t theo yÃªu cáº§u
- GiÃ¡n Ä‘oáº¡n thÃ´ng minh - cho phÃ©p ngÆ°á»i dÃ¹ng Ä‘áº·t cÃ¢u há»i tiáº¿p theo giá»¯a chá»«ng
- Báº£o tá»“n ngá»¯ cáº£nh - duy trÃ¬ tráº¡ng thÃ¡i cuá»™c trÃ² chuyá»‡n qua cÃ¡c lá»—i
- Streaming thÃ­ch á»©ng - Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ dá»±a trÃªn tá»‘c Ä‘á»™ Ä‘á»c cá»§a ngÆ°á»i dÃ¹ng
- Pháº£n há»“i Ä‘a phÆ°Æ¡ng thá»©c - káº¿t há»£p vÄƒn báº£n, hÃ¬nh áº£nh vÃ  cÃ¡c yáº¿u tá»‘ tÆ°Æ¡ng tÃ¡c

ðŸ”§ **Tá»‘i Æ¯u Ká»¹ Thuáº­t:**
- PhÃ¢n Ä‘oáº¡n pháº£n há»“i - chia cÃ¡c pháº£n há»“i dÃ i thÃ nh cÃ¡c pháº§n logic
- Xá»­ lÃ½ song song - xá»­ lÃ½ nhiá»u luá»“ng Ä‘á»“ng thá»i
- Äá»‡m thÃ´ng minh - tá»‘i Æ°u hÃ³a viá»‡c sá»­ dá»¥ng máº¡ng
- NÃ©n - giáº£m viá»‡c sá»­ dá»¥ng bÄƒng thÃ´ng cho cÃ¡c pháº£n há»“i lá»›n
- TÃ­ch há»£p CDN - phÃ¢n phá»‘i táº£i streaming trÃªn toÃ n cáº§u

### BÆ°á»›c Tiáº¿p Theo

Trong **bÃ i tiáº¿p theo**, chÃºng ta sáº½ khÃ¡m phÃ¡:

ðŸ­ **Production Best Practices vÃ  Performance Optimization** - Triá»ƒn khai vÃ  tá»‘i Æ°u hÃ³a toÃ n diá»‡n  
ðŸ”§ **Advanced Architecture Patterns** - Kiáº¿n trÃºc cÃ³ thá»ƒ má»Ÿ rá»™ng cho enterprise systems  
ðŸ“ˆ **Cost Optimization Strategies** - Tá»‘i Æ°u chi phÃ­ trong production environments  

### Thá»­ ThÃ¡ch DÃ nh Cho Báº¡n

1. **XÃ¢y dá»±ng há»‡ thá»‘ng giÃ¡m sÃ¡t toÃ n diá»‡n** vá»›i dashboard thá»i gian thá»±c
2. **Triá»ƒn khai intelligent error recovery** vá»›i báº£o tá»“n ngá»¯ cáº£nh
3. **Táº¡o performance analysis tools** Ä‘á»ƒ tá»‘i Æ°u hÃ³a agent behavior
4. **Thiáº¿t káº¿ troubleshooting automation** cho cÃ¡c váº¥n Ä‘á» thÆ°á»ng gáº·p
5. **PhÃ¡t triá»ƒn monitoring alerts** thÃ­ch á»©ng vá»›i cÃ¡c máº«u sá»­ dá»¥ng khÃ¡c nhau

---

*BÃ i tiáº¿p theo: [**"Production Best Practices vÃ  Performance Optimization"**](../openai-agents-sdk-part-10-production-best-practices-optimization/) - ChÃºng ta sáº½ há»c cÃ¡ch triá»ƒn khai vÃ  tá»‘i Æ°u hÃ³a agent systems cho production environments vá»›i performance cao vÃ  reliability.*